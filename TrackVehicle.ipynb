{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Self-Driving Car Engineer Nanodegree\n",
    "\n",
    "\n",
    "## Project: **Vehicle Detection and Tracking** \n",
    "***\n",
    "\n",
    "This jupyter notebook contains code to train a classifier to identify car images taken from the front camera and apply to classifier to detect and track cars in a video stream\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Import necessary packages\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import time\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from skimage.feature import hog\n",
    "from support_functions import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Extract training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Extract training data\n",
    "\n",
    "# Read in cars and notcars\n",
    "cars = glob.glob('training_images\\\\vehicles\\\\**\\\\*.png', recursive=True)\n",
    "notcars = glob.glob('training_images\\\\non-vehicles\\\\**\\\\*.png', recursive=True)\n",
    "training_set = cars + notcars\n",
    "print(\"Number of car images = \", len(cars))\n",
    "print(\"Number of non car images = \", len(notcars))\n",
    "print(\"Total number of images in training set = \", len(training_set))\n",
    "print(\" \")\n",
    "\n",
    "# Define the labels vector\n",
    "y = np.hstack((np.ones(len(cars)), np.zeros(len(notcars))))\n",
    "\n",
    "# Check if the training dataset is distributed evenly across car and not-car images\n",
    "labels = 'Cars', 'Non Cars'\n",
    "sizes = [len(cars), len(notcars)]\n",
    "explode = (0, 0.05)\n",
    "p, texts, autotexts = plt.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "plt.title('Vehicle Image distribution', bbox={'facecolor':'0.8', 'pad':5})\n",
    "texts[0].set_fontsize(20)\n",
    "texts[1].set_fontsize(20)\n",
    "autotexts[0].set_fontsize(15)\n",
    "autotexts[1].set_fontsize(15)\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "if 0.5 < (len(cars)/len(notcars)) < 2:\n",
    "    print(\"**********  Training data is balanced  **********\")\n",
    "else:\n",
    "    print(\"**********  Training data is not balanced  **********\")\n",
    "print(\" \")\n",
    "    \n",
    "# Do a sanity check by picking random images to make sure data is not corrupt\n",
    "i = 250\n",
    "print(cars[i])\n",
    "car = cv2.imread(cars[i])\n",
    "car = cv2.cvtColor(car, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "print(notcars[i])\n",
    "noncar = cv2.imread(notcars[i])\n",
    "noncar = cv2.cvtColor(noncar, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "f = plt.figure(figsize=(40, 40))\n",
    "ax1 = plt.subplot(1, 3, 1)\n",
    "ax1.imshow(car)\n",
    "ax1.set_title('Car Image', fontsize=40)\n",
    "plt.xticks([0,10,20,30,40,50,60], fontsize = 30) \n",
    "plt.yticks([0,10,20,30,40,50,60], fontsize = 30) \n",
    "ax2 = plt.subplot(1, 3, 2)\n",
    "ax2.imshow(noncar)\n",
    "ax2.set_title('Not Car', fontsize=40)\n",
    "plt.xticks([0,10,20,30,40,50,60], fontsize = 30) \n",
    "plt.yticks([0,10,20,30,40,50,60], fontsize = 30) \n",
    "f.savefig('car_not_car.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Define feature extraction parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Define feature extraction parameters\n",
    "\n",
    "color_space = 'YCrCb'     # Can be RGB, HSV, LUV, HLS, YUV, YCrCb\n",
    "orient = 9                # HOG orientations\n",
    "pix_per_cell = 8          # HOG pixels per cell\n",
    "cell_per_block = 2        # HOG cells per block\n",
    "hog_channel = \"ALL\"       # Can be 0, 1, 2, or \"ALL\"\n",
    "spatial_size = (32, 32)   # Spatial binning dimensions\n",
    "hist_bins = 32            # Number of histogram bins\n",
    "spatial_feat = True       # Spatial features on or off\n",
    "hist_feat = True          # Histogram features on or off\n",
    "hog_feat = True           # HOG features on or off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Extract features for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Extract features\n",
    "spatial_features_sample = []\n",
    "color_features_sample = []\n",
    "hog_features_sample = []\n",
    "def extract_features(imgs, color_space='RGB', spatial_size=(32, 32),\n",
    "                        hist_bins=32, orient=9, \n",
    "                        pix_per_cell=8, cell_per_block=2, hog_channel=0,\n",
    "                        spatial_feat=True, hist_feat=True, hog_feat=True):\n",
    "    global spatial_features_sample, color_features_sample, hog_features_sample\n",
    "    # Start timer\n",
    "    t=time.time()\n",
    "    # Create a list to append feature vectors to\n",
    "    features = []\n",
    "    # Iterate through the list of images\n",
    "    for file in imgs:\n",
    "        file_features = []\n",
    "        # Read in each one by one\n",
    "        image = cv2.imread(file)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        # apply color conversion if other than 'RGB'\n",
    "        if color_space != 'RGB':\n",
    "            feature_image = convert_color(image, color_space)\n",
    "        else: feature_image = np.copy(image)      \n",
    "\n",
    "        if spatial_feat == True:\n",
    "            spatial_features = bin_spatial(feature_image, size=spatial_size)\n",
    "            file_features.append(spatial_features)\n",
    "            spatial_features_sample = spatial_features\n",
    "        if hist_feat == True:\n",
    "            # Apply color_hist()\n",
    "            hist_features = color_hist(feature_image, nbins=hist_bins)\n",
    "            file_features.append(hist_features)\n",
    "            color_features_sample = hist_features\n",
    "        if hog_feat == True:\n",
    "        # Call get_hog_features() with vis=False, feature_vec=True\n",
    "            if hog_channel == 'ALL':\n",
    "                hog_features = []\n",
    "                for channel in range(feature_image.shape[2]):\n",
    "                    hog_features.append(get_hog_features(feature_image[:,:,channel], \n",
    "                                        orient, pix_per_cell, cell_per_block, \n",
    "                                        vis=False, feature_vec=True))\n",
    "                hog_features = np.ravel(hog_features)        \n",
    "            else:\n",
    "                hog_features = get_hog_features(feature_image[:,:,hog_channel], orient, \n",
    "                            pix_per_cell, cell_per_block, vis=False, feature_vec=True)\n",
    "            # Append the new feature vector to the features list\n",
    "            file_features.append(hog_features)\n",
    "            hog_features_sample = hog_features\n",
    "        features.append(np.concatenate(file_features))\n",
    "    # Stop timer\n",
    "    t2 = time.time()\n",
    "    print(round(t2-t, 2), 'Seconds to extract features...')\n",
    "    # Return list of feature vectors\n",
    "    return features\n",
    "\n",
    "features = extract_features(training_set, color_space=color_space, \n",
    "                        spatial_size=spatial_size, hist_bins=hist_bins, \n",
    "                        orient=orient, pix_per_cell=pix_per_cell, \n",
    "                        cell_per_block=cell_per_block, \n",
    "                        hog_channel=hog_channel, spatial_feat=spatial_feat, \n",
    "                        hist_feat=hist_feat, hog_feat=hog_feat)\n",
    "\n",
    "# Fit a per-column scaler\n",
    "X_scaler = StandardScaler().fit(features)\n",
    "# Apply the scaler to X\n",
    "scaled_X = X_scaler.transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Visualize feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Visualize Feature Extraction\n",
    "\n",
    "car = cv2.imread(cars[i])\n",
    "car = cv2.cvtColor(car, cv2.COLOR_BGR2RGB)\n",
    "noncar = cv2.imread(notcars[i])\n",
    "noncar = cv2.cvtColor(noncar, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#Convert color space to YCrCb\n",
    "car = cv2.cvtColor(car, cv2.COLOR_RGB2YCrCb)\n",
    "noncar = cv2.cvtColor(noncar, cv2.COLOR_RGB2YCrCb)\n",
    "\n",
    "# Extract HOG image for 1 channel (Y)\n",
    "chog_features, chog_image = get_hog_features(car[:,:,0], orient, pix_per_cell, cell_per_block, \n",
    "                                               vis=True, feature_vec=True)\n",
    "nhog_features, nhog_image = get_hog_features(noncar[:,:,0], orient, pix_per_cell, cell_per_block, \n",
    "                                               vis=True, feature_vec=True)\n",
    "\n",
    "# Extract spatial bin on 3 channels seperately\n",
    "ccolor1 = cv2.resize(car[:,:,0], spatial_size)\n",
    "ccolor2 = cv2.resize(car[:,:,1], spatial_size)\n",
    "ccolor3 = cv2.resize(car[:,:,2], spatial_size)\n",
    "\n",
    "ncolor1 = cv2.resize(noncar[:,:,0], spatial_size)\n",
    "ncolor2 = cv2.resize(noncar[:,:,1], spatial_size)\n",
    "ncolor3 = cv2.resize(noncar[:,:,2], spatial_size)\n",
    "\n",
    "f = plt.figure(figsize=(60, 60))\n",
    "ax1 = plt.subplot(4, 4, 1)\n",
    "ax1.imshow(car[:,:,0], cmap='gray')\n",
    "ax1.set_title('Car Channel 0', fontsize=40)\n",
    "plt.xticks([0,10,20,30,40,50,60], fontsize = 30) \n",
    "plt.yticks([0,10,20,30,40,50,60], fontsize = 30) \n",
    "ax2 = plt.subplot(4, 4, 2)\n",
    "ax2.imshow(chog_image, cmap='gray')\n",
    "ax2.set_title('Car Ch-0 HOG', fontsize=40)\n",
    "plt.xticks([0,10,20,30,40,50,60], fontsize = 30) \n",
    "plt.yticks([0,10,20,30,40,50,60], fontsize = 30) \n",
    "ax3 = plt.subplot(4, 4, 3)\n",
    "ax3.imshow(noncar[:,:,0], cmap='gray')\n",
    "ax3.set_title('Not Car Channel 0', fontsize=40)\n",
    "plt.xticks([0,10,20,30,40,50,60], fontsize = 30) \n",
    "plt.yticks([0,10,20,30,40,50,60], fontsize = 30) \n",
    "ax4 = plt.subplot(4, 4, 4)\n",
    "ax4.imshow(nhog_image, cmap=\"gray\")\n",
    "ax4.set_title('Not Car Ch-0 HOG', fontsize=40)\n",
    "plt.xticks([0,10,20,30,40,50,60], fontsize = 30) \n",
    "plt.yticks([0,10,20,30,40,50,60], fontsize = 30) \n",
    "ax5 = plt.subplot(4, 4, 5)\n",
    "ax5.imshow(car[:,:,0], cmap=\"gray\")\n",
    "ax5.set_title('Car Channel 0', fontsize=40)\n",
    "plt.xticks([0,10,20,30,40,50,60], fontsize = 30) \n",
    "plt.yticks([0,10,20,30,40,50,60], fontsize = 30) \n",
    "ax6 = plt.subplot(4, 4, 6)\n",
    "ax6.imshow(ccolor1, cmap=\"gray\")\n",
    "ax6.set_title('Car Ch-0 Features', fontsize=40)\n",
    "plt.xticks([0,10,20,30,32], fontsize = 30) \n",
    "plt.yticks([0,10,20,30,32], fontsize = 30) \n",
    "ax7 = plt.subplot(4, 4, 7)\n",
    "ax7.imshow(noncar[:,:,0], cmap=\"gray\")\n",
    "ax7.set_title('Not Car Channel 0', fontsize=40)\n",
    "plt.xticks([0,10,20,30,40,50,60], fontsize = 30) \n",
    "plt.yticks([0,10,20,30,40,50,60], fontsize = 30) \n",
    "ax8 = plt.subplot(4, 4, 8)\n",
    "ax8.imshow(ncolor1, cmap=\"gray\")\n",
    "ax8.set_title('Not Car Ch-0 Features', fontsize=40)\n",
    "plt.xticks([0,10,20,30,32], fontsize = 30) \n",
    "plt.yticks([0,10,20,30,32], fontsize = 30) \n",
    "ax9 = plt.subplot(4, 4, 9)\n",
    "ax9.imshow(car[:,:,1], cmap=\"gray\")\n",
    "ax9.set_title('Car Channel 1', fontsize=40)\n",
    "plt.xticks([0,10,20,30,40,50,60], fontsize = 30) \n",
    "plt.yticks([0,10,20,30,40,50,60], fontsize = 30) \n",
    "ax10 = plt.subplot(4, 4, 10)\n",
    "ax10.imshow(ccolor2, cmap=\"gray\")\n",
    "ax10.set_title('Car Ch-1 Features', fontsize=40)\n",
    "plt.xticks([0,10,20,30,32], fontsize = 30) \n",
    "plt.yticks([0,10,20,30,32], fontsize = 30) \n",
    "ax11 = plt.subplot(4, 4, 11)\n",
    "ax11.imshow(noncar[:,:,1], cmap=\"gray\")\n",
    "ax11.set_title('Not Car Channel 1', fontsize=40)\n",
    "plt.xticks([0,10,20,30,40,50,60], fontsize = 30) \n",
    "plt.yticks([0,10,20,30,40,50,60], fontsize = 30) \n",
    "ax12 = plt.subplot(4, 4, 12)\n",
    "ax12.imshow(ncolor2, cmap=\"gray\")\n",
    "ax12.set_title('Not Car Ch-1 Features', fontsize=40)\n",
    "plt.xticks([0,10,20,30,32], fontsize = 30) \n",
    "plt.yticks([0,10,20,30,32], fontsize = 30) \n",
    "ax13 = plt.subplot(4, 4, 13)\n",
    "ax13.imshow(car[:,:,2], cmap=\"gray\")\n",
    "ax13.set_title('Car Channel 2', fontsize=40)\n",
    "plt.xticks([0,10,20,30,40,50,60], fontsize = 30) \n",
    "plt.yticks([0,10,20,30,40,50,60], fontsize = 30) \n",
    "ax14 = plt.subplot(4, 4, 14)\n",
    "ax14.imshow(ccolor3, cmap=\"gray\")\n",
    "ax14.set_title('Car Ch-2 Features', fontsize=40)\n",
    "plt.xticks([0,10,20,30,32], fontsize = 30) \n",
    "plt.yticks([0,10,20,30,32], fontsize = 30) \n",
    "ax15 = plt.subplot(4, 4, 15)\n",
    "ax15.imshow(noncar[:,:,2], cmap=\"gray\")\n",
    "ax15.set_title('Not Car Channel 2', fontsize=40)\n",
    "plt.xticks([0,10,20,30,40,50,60], fontsize = 30) \n",
    "plt.yticks([0,10,20,30,40,50,60], fontsize = 30) \n",
    "ax16 = plt.subplot(4, 4, 16)\n",
    "ax16.imshow(ncolor3, cmap=\"gray\")\n",
    "ax16.set_title('Not Car Ch-2 Features', fontsize=40)\n",
    "plt.xticks([0,10,20,30,32], fontsize = 30) \n",
    "plt.yticks([0,10,20,30,32], fontsize = 30) \n",
    "f.savefig('FeatureVisuvalization.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "###  Train the model\n",
    "\n",
    "# Split up data into randomized training and test sets\n",
    "rand_state = np.random.randint(0, 100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    scaled_X, y, test_size=0.2, random_state=rand_state)\n",
    "\n",
    "print('Using:',orient,'orientations',pix_per_cell, 'pixels per cell and', cell_per_block,'cells per block')\n",
    "print('Feature vector length:', len(X_train[0]))\n",
    "\n",
    "# Use a linear SVC \n",
    "svc = LinearSVC()\n",
    "# Check the training time for the SVC\n",
    "t=time.time()\n",
    "svc.fit(X_train, y_train)\n",
    "t2 = time.time()\n",
    "print(round(t2-t, 2), 'Seconds to train SVC...')\n",
    "# Check the score of the SVC\n",
    "print('Test Accuracy of SVC = ', round(svc.score(X_test, y_test), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Save model and feature extraction parameters to pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Save fit and feature extraction parameters\n",
    "\n",
    "pickle_file = 'VDT_pickle.p'\n",
    "print('Saving data to pickle file...')\n",
    "try:\n",
    "    with open('VDT_pickle.p', 'wb') as pfile:\n",
    "        pickle.dump(\n",
    "            {\n",
    "                'svc': svc,\n",
    "                'X_scaler': X_scaler,\n",
    "                'color_space': color_space,\n",
    "                'hog_channel': hog_channel,\n",
    "                'orient': orient,\n",
    "                'pix_per_cell': pix_per_cell,\n",
    "                'cell_per_block': cell_per_block,\n",
    "                'spatial_size': spatial_size,\n",
    "                'hist_bins': hist_bins,\n",
    "            },\n",
    "            pfile, pickle.HIGHEST_PROTOCOL)\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', pickle_file, ':', e)\n",
    "    raise\n",
    "print(\"Pickle file saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Identifying vehicles using the trained classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import cv2\n",
    "from support_functions import *\n",
    "from scipy.ndimage.measurements import label\n",
    "import glob\n",
    "\n",
    "\n",
    "class car_tracker():\n",
    "    def __init__(self):\n",
    "        self.frame_number = None\n",
    "        self.cars_located = None\n",
    "        self.detected_car_positions = None\n",
    "        self.heat = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Load the classifier and feature extraction parameters from the pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svc =  LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "scaler =  StandardScaler(copy=True, with_mean=True, with_std=True)\n",
      "color space =  YCrCb\n",
      "hog channel =  ALL\n",
      "orient =  9\n",
      "pix_per_cell =  8\n",
      "cell_per_block =  2\n",
      "spatial_size =  (32, 32)\n",
      "hist_bins =  32\n"
     ]
    }
   ],
   "source": [
    "dist_pickle = pickle.load( open(\"VDT_pickle.p\", \"rb\" ) )\n",
    "svc = dist_pickle[\"svc\"]\n",
    "X_scaler = dist_pickle[\"X_scaler\"]\n",
    "color_space = dist_pickle[\"color_space\"]\n",
    "hog_channel = dist_pickle[\"hog_channel\"]\n",
    "orient = dist_pickle[\"orient\"]\n",
    "pix_per_cell = dist_pickle[\"pix_per_cell\"]\n",
    "cell_per_block = dist_pickle[\"cell_per_block\"]\n",
    "spatial_size = dist_pickle[\"spatial_size\"]\n",
    "hist_bins = dist_pickle[\"hist_bins\"]\n",
    "\n",
    "print(\"svc = \", svc)\n",
    "print(\"scaler = \", X_scaler)\n",
    "print(\"color space = \", color_space)\n",
    "print(\"hog channel = \", hog_channel)\n",
    "print(\"orient = \", orient)\n",
    "print(\"pix_per_cell = \", pix_per_cell)\n",
    "print(\"cell_per_block = \", cell_per_block)\n",
    "print(\"spatial_size = \", spatial_size)\n",
    "print(\"hist_bins = \", hist_bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Vehicle Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define a single function that can extract features using hog sub-sampling and make predictions\n",
    "def find_cars(img, ystart, ystop, scale, svc, X_scaler, orient, pix_per_cell, cell_per_block, spatial_size, hist_bins, color_space):\n",
    "    \n",
    "    bboxes = []\n",
    "    draw_img = np.copy(img)\n",
    "#    img = img.astype(np.float32)/255\n",
    "\n",
    "    img_tosearch = img[ystart:ystop,:,:]\n",
    "    ctrans_tosearch = convert_color(img_tosearch, color_space)\n",
    "#    ctrans_tosearch = cv2.cvtColor(img_tosearch, cv2.COLOR_RGB2YCrCb)\n",
    "    if scale != 1:\n",
    "        imshape = ctrans_tosearch.shape\n",
    "        ctrans_tosearch = cv2.resize(ctrans_tosearch, (np.int(imshape[1]/scale), np.int(imshape[0]/scale)))\n",
    "        \n",
    "    ch1 = ctrans_tosearch[:,:,0]\n",
    "    ch2 = ctrans_tosearch[:,:,1]\n",
    "    ch3 = ctrans_tosearch[:,:,2]\n",
    "\n",
    "    # Define blocks and steps as above\n",
    "#    print(\"ch1.shape[1] = \", ch1.shape[1])\n",
    "    nxblocks = (ch1.shape[1] // pix_per_cell) - cell_per_block + 1\n",
    "#    print(\"nxblocks =\", nxblocks)\n",
    "    nyblocks = (ch1.shape[0] // pix_per_cell) - cell_per_block + 1 \n",
    "#    print(\"nyblocks =\", nyblocks)\n",
    "    nfeat_per_block = orient*cell_per_block**2\n",
    "#    print(\"nfeatpb =\", nfeat_per_block)\n",
    "    \n",
    "    # 64 was the orginal sampling rate, with 8 cells and 8 pix per cell\n",
    "    window = 64\n",
    "    nblocks_per_window = (window // pix_per_cell) - cell_per_block + 1\n",
    "#    print(\"nblocks_per_window = \", nblocks_per_window)\n",
    "    cells_per_step = 2  # Instead of overlap, define how many cells to step\n",
    "    nxsteps = (nxblocks - nblocks_per_window) // cells_per_step\n",
    "    nysteps = (nyblocks - nblocks_per_window) // cells_per_step\n",
    "#    print(\"nxsteps = \", nxsteps)\n",
    "#    print(\"nysteps = \", nysteps)\n",
    "    \n",
    "    # Compute individual channel HOG features for the entire image\n",
    "    hog1 = get_hog_features(ch1, orient, pix_per_cell, cell_per_block, feature_vec=False)\n",
    "    hog2 = get_hog_features(ch2, orient, pix_per_cell, cell_per_block, feature_vec=False)\n",
    "    hog3 = get_hog_features(ch3, orient, pix_per_cell, cell_per_block, feature_vec=False)\n",
    "    \n",
    "    for xb in range(nxsteps):\n",
    "        for yb in range(nysteps):\n",
    "            ypos = yb*cells_per_step\n",
    "            xpos = xb*cells_per_step\n",
    "            # Extract HOG for this patch\n",
    "            hog_feat1 = hog1[ypos:ypos+nblocks_per_window, xpos:xpos+nblocks_per_window].ravel() \n",
    "            hog_feat2 = hog2[ypos:ypos+nblocks_per_window, xpos:xpos+nblocks_per_window].ravel() \n",
    "            hog_feat3 = hog3[ypos:ypos+nblocks_per_window, xpos:xpos+nblocks_per_window].ravel() \n",
    "            hog_features = np.hstack((hog_feat1, hog_feat2, hog_feat3))\n",
    "\n",
    "            xleft = xpos*pix_per_cell\n",
    "            ytop = ypos*pix_per_cell\n",
    "\n",
    "            # Extract the image patch\n",
    "            subimg = cv2.resize(ctrans_tosearch[ytop:ytop+window, xleft:xleft+window], (64,64))\n",
    "          \n",
    "            # Get color features\n",
    "            spatial_features = bin_spatial(subimg, size=spatial_size)\n",
    "            hist_features = color_hist(subimg, nbins=hist_bins)\n",
    "\n",
    "            # Fit a per-column scaler\n",
    "            test_features = X_scaler.transform(np.hstack((spatial_features, hist_features, hog_features)).reshape(1, -1))    \n",
    "            # Scale features and make a prediction\n",
    "            test_prediction = svc.predict(test_features)\n",
    "            \n",
    "            if test_prediction == 1:\n",
    "                xbox_left = np.int(xleft*scale)\n",
    "                ytop_draw = np.int(ytop*scale)\n",
    "                win_draw = np.int(window*scale)\n",
    "#                cv2.rectangle(draw_img,(xbox_left, ytop_draw+ystart),(xbox_left+win_draw,ytop_draw+win_draw+ystart),(0,0,255),6) \n",
    "                box = [(xbox_left, ytop_draw+ystart), (xbox_left+win_draw,ytop_draw+win_draw+ystart)]\n",
    "                bboxes.append(box)\n",
    "                \n",
    "    return bboxes \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Vehicle Detection Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def detection_pipeline(img):\n",
    "    ystart = 400\n",
    "    ystop = 656\n",
    "    scale_list = [1, 1.5, 2, 2.5]\n",
    "    bboxes = []\n",
    "    heat_threshold = 5\n",
    "    global track\n",
    "\n",
    "    for scale in scale_list:\n",
    "        detected_boxes = find_cars(img, ystart, ystop, scale, svc, X_scaler, orient, pix_per_cell, cell_per_block, \n",
    "                            spatial_size, hist_bins, color_space)\n",
    "        bboxes.extend(detected_boxes)\n",
    "\n",
    "    out_img = np.copy(img)\n",
    "    for box in bboxes:\n",
    "        cv2.rectangle(out_img, box[0], box[1],(0,0,255),6) \n",
    "\n",
    "    out_img = cv2.cvtColor(out_img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    heat = np.zeros_like(img[:,:,0]).astype(np.float)\n",
    "\n",
    "    # Add heat to each box in box list\n",
    "    heat = add_heat(heat,bboxes)\n",
    "    heat_holder = np.copy(heat)\n",
    "    \n",
    "    i = 1\n",
    "    # Smoothen the heat over the last n frames\n",
    "    for prev_frame in tracker[::-1]:\n",
    "        heat = heat + prev_frame.heat\n",
    "        if i == 5: \n",
    "            heat = heat / i\n",
    "            break \n",
    "        i += 1\n",
    "    track.heat = heat_holder\n",
    "\n",
    "    # Apply threshold to help remove false positives\n",
    "    heat = apply_threshold(heat, heat_threshold)\n",
    "\n",
    "    # Visualize the heatmap when displaying    \n",
    "    heatmap = np.clip(heat, 0, 255)\n",
    "    \n",
    "    # Find final boxes from heatmap using label function\n",
    "    labels = label(heatmap)\n",
    "    draw_img, final_bboxes = draw_labeled_bboxes(np.copy(img), labels)\n",
    "    track.cars_located = labels\n",
    "    track.detected_car_positions = final_bboxes\n",
    "    cv2.putText(draw_img,\"Frame Number  = \" + str(track.frame_number), (900,50), cv2.FONT_HERSHEY_PLAIN, 2, (255,255,255))\n",
    "    cv2.putText(draw_img,\"Cars detected =  \" + str(labels[1]), (900,80), cv2.FONT_HERSHEY_PLAIN, 2, (255,255,255))\n",
    "\n",
    "    draw_img = cv2.cvtColor(draw_img, cv2.COLOR_RGB2BGR)\n",
    "#    cv2.imwrite(\"draw_img\" + str(track.frame_number) + \".jpg\",draw_img)\n",
    "    label_img = np.dstack((labels[0], labels[0], labels[0]))*255\n",
    "#    cv2.imwrite(\"labels_img\" + str(track.frame_number) + \".jpg\",label_img)\n",
    "    \n",
    "    return draw_img, heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Test vehicle detection pipeline on test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "image = cv2.imread('test_images\\\\test6*.jpg')\n",
    "tracker = []\n",
    "track = car_tracker()\n",
    "track.frame_number = 1\n",
    "track.cars_located = 0\n",
    "track.detected_car_positions = []\n",
    "track.heat = np.zeros_like(image).astype(np.float)\n",
    "tracker.append(track)\n",
    "\n",
    "test_images = glob.glob('test_images\\\\test5*')\n",
    "for image in test_images:\n",
    "    img = cv2.imread(image)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    draw_img, heatmap = detection_pipeline(img)\n",
    "    draw_img = cv2.cvtColor(draw_img, cv2.COLOR_BGR2RGB)\n",
    "    fig = plt.figure(figsize=(50, 30))\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(draw_img)\n",
    "    plt.title(image, fontsize=40)\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(heatmap, cmap='hot')\n",
    "    plt.title('Heat Map', fontsize=40)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    draw_img = cv2.cvtColor(draw_img, cv2.COLOR_RGB2BGR)\n",
    "    cv2.imwrite(\"marked_img.jpg\",draw_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Run vehicle detection pipeline on videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Import everything needed to edit/save/watch video clips\n",
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "def process_image(image):  \n",
    "    global track\n",
    "    track.frame_number = tracker[-1].frame_number + 1\n",
    "    result, heatmap = detection_pipeline(image)\n",
    "    tracker.append(track)\n",
    "    track = car_tracker()\n",
    "    result = cv2.cvtColor(result, cv2.COLOR_RGB2BGR)\n",
    "    result = lane_marker_pipeline(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "image = cv2.imread('test_images\\\\test6*.jpg')\n",
    "tracker = []\n",
    "track = car_tracker()\n",
    "track.frame_number = 1\n",
    "track.cars_located = 0\n",
    "track.detected_car_positions = []\n",
    "track.heat = np.zeros_like(image).astype(np.float)\n",
    "tracker.append(track)\n",
    "test_video_output = 'test_video_output.mp4'\n",
    "#clip2 = VideoFileClip('test_video.mp4')\n",
    "clip2 = VideoFileClip('t4.mp4')\n",
    "video_clip = clip2.fl_image(process_image)\n",
    "%time video_clip.write_videofile(test_video_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "image = cv2.imread('test_images\\\\test6*.jpg')\n",
    "tracker = []\n",
    "track = car_tracker()\n",
    "track.frame_number = 1\n",
    "track.cars_located = 0\n",
    "track.detected_car_positions = []\n",
    "track.heat = np.zeros_like(image).astype(np.float)\n",
    "tracker.append(track)\n",
    "project_video_output = 'project_video_output.mp4'\n",
    "clip2 = VideoFileClip('project_video.mp4')\n",
    "video_clip = clip2.fl_image(process_image)\n",
    "%time video_clip.write_videofile(project_video_output, audio=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Lane Finding code section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All package import complete\n"
     ]
    }
   ],
   "source": [
    "### Importing necessary packages\n",
    "\n",
    "import numpy as np                  # For handling images\n",
    "import glob                         # For iterating through multiple image files\n",
    "import cv2                          # For image manipulation\n",
    "import matplotlib.pyplot as plt     # For plotting images\n",
    "\n",
    "# For plotting to display inline within the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"All package import complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All global variable definition complete\n"
     ]
    }
   ],
   "source": [
    "### Global variables and constants defined here\n",
    "\n",
    "detected_lines = []                  # Global variable to hold all detected lines\n",
    "reset_lanes = True                   # Flag that drives detection of fresh lanes\n",
    "terrain_type = None                  # Variable that indicates the terrain type\n",
    "mtx = dist = None                    # Camera caliberation parameters\n",
    "perspective_M = perspective_Minv = None  # Perspective and inverse perspective\n",
    "baseline_lane_width = None           # Baseline lane width in px calculated from reference image\n",
    "cb = np.empty([360,1930,3])          # Holding variable for image concatenation        \n",
    "c1 = np.empty([360,640,3])           # Holding variable for image concatenation\n",
    "c2 = np.empty([370,640,3])           # Holding variable for image concatenation\n",
    "\n",
    "class Line():                        # Objects of this Line class hold details of a detected lines\n",
    "    def __init__(self):\n",
    "\n",
    "        self.fresh_lane = None\n",
    "        self.good_lane = None\n",
    "        self.continous_bad_lanes = 0\n",
    "        self.leftx_base = None\n",
    "        self.rightx_base = None\n",
    "        self.leftx = None\n",
    "        self.lefty = None\n",
    "        self.rightx = None\n",
    "        self.righty = None\n",
    "        self.left_fit = None\n",
    "        self.right_fit = None\n",
    "        self.left_curverad = None\n",
    "        self.right_curverad = None\n",
    "        self.center_offset_m = None\n",
    "        self.lane_width = None\n",
    "        self.top_lane_width = None\n",
    "        self.smooth_leftx = 0\n",
    "        self.smooth_rightx = 0\n",
    "        self.smooth_leftx_base = 0\n",
    "        self.smooth_rightx_base = 0\n",
    "        self.smooth_left_fit = [0,0,0]\n",
    "        self.smooth_right_fit = [0,0,0]\n",
    "\n",
    "print(\"All global variable definition complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Support Functions\n",
    "\n",
    "\"\"\" The helper functions needed to identify lanes are defined in this section \"\"\"\n",
    "\n",
    "def showImg(img):\n",
    "    \"\"\" Combines plt.imshow() and plt.show() so that\n",
    "        grayscale images can be displayed with a single line funtion call\"\"\"\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def showGrayImg(img):\n",
    "    \"\"\" Combines plt.imshow() and plt.show() so that \n",
    "        images can be displayed with a single line funtion call\"\"\"\n",
    "    plt.imshow(img, cmap=\"gray\")\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def undistort(img):\n",
    "    \"\"\" Undistorts a given image using \n",
    "        Camera caliberations\"\"\"\n",
    "    return cv2.undistort(img, mtx, dist, None, mtx)\n",
    "\n",
    "def grayscale(img):\n",
    "    \"\"\"Applies the Grayscale transform\n",
    "       This will return an image with only one color channel\"\"\"\n",
    "    \n",
    "    return cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "def check_lighting(img):\n",
    "    \"\"\"Checks for low luminosity (shadow or cloudy conditions)\n",
    "       This will return \"Good\" if good lighting else return \"Poor\" \"\"\"\n",
    "\n",
    "    img = cv2.cvtColor(img,cv2.COLOR_RGB2HLS)\n",
    "    img_luminosity = img[430:,:,1]\n",
    "    if np.average(img_luminosity) < 50:\n",
    "        lighting_condition = \"Poor\"\n",
    "        drive_log.write(\"@ \" + str(len(detected_lines)) + \" - Switched ON headlights \" + \n",
    "                        str(np.average(img_luminosity)) + \"\\n\")\n",
    "    else:\n",
    "        lighting_condition = \"Good\"\n",
    "#    print(np.average(img_luminosity))\n",
    "    \n",
    "    return lighting_condition\n",
    "\n",
    "def switch_on_headlights(img):\n",
    "    \"\"\"Illuminates lower half of the image by increasing luminosity\n",
    "       Returns the input image with bottom section illuminated \"\"\"\n",
    "  \n",
    "    illuminated_image = cv2.cvtColor(img,cv2.COLOR_RGB2HLS)\n",
    "    illuminated_image = np.int16(illuminated_image)\n",
    "    illuminated_image[430:,:,1] = np.clip(illuminated_image[430:,:,1] * 2, 0, 255)\n",
    "    illuminated_image[430:,:,2] = np.clip(illuminated_image[430:,:,2] * 3, 0, 255)\n",
    "    illuminated_image = np.uint8(illuminated_image)\n",
    "    illuminated_image = cv2.cvtColor(illuminated_image, cv2.COLOR_HLS2RGB)\n",
    "    \n",
    "    return illuminated_image\n",
    "\n",
    "def gradient(gray, orient, sobel_kernel, thresh):\n",
    "    \"\"\" Calculates directional gradient in the required orientation\n",
    "        Applies threshold\"\"\"\n",
    "    \n",
    "    if orient == 'x':\n",
    "        sobel = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n",
    "    else:\n",
    "        sobel = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n",
    "\n",
    "    abs_sobel = np.absolute(sobel)\n",
    "    scaled_sobel = np.uint8(255*abs_sobel/np.max(abs_sobel))\n",
    "    grad_binary = np.zeros_like(scaled_sobel)\n",
    "    grad_binary[(scaled_sobel >= thresh[0]) & (scaled_sobel <= thresh[1])] = 1\n",
    "    return grad_binary\n",
    "\n",
    "def gradient_mag(gray, sobel_kernel, mag_thresh):\n",
    "    \"\"\" Calculates gradient magnitude\n",
    "        Applies threshold\"\"\"\n",
    "\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n",
    "    abs_sobel = np.sqrt( (sobelx*sobelx) + (sobely*sobely) )\n",
    "    scaled_sobel = np.uint8(255*abs_sobel/np.max(abs_sobel))\n",
    "    mag_binary = np.zeros_like(scaled_sobel)\n",
    "    mag_binary[(scaled_sobel >= mag_thresh[0]) & (scaled_sobel <= mag_thresh[1])] = 1\n",
    "\n",
    "    return mag_binary\n",
    "\n",
    "def gradient_dir(gray, sobel_kernel, dir_thresh):\n",
    "    \"\"\" Calculates gradient direction\n",
    "        Applies threshold\"\"\"\n",
    "    \n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n",
    "    abs_sobelx = np.absolute(sobelx)\n",
    "    abs_sobely = np.absolute(sobely)\n",
    "    dir_grad = np.arctan2(abs_sobely, abs_sobelx)\n",
    "#    scaled_dir_grad = np.uint8(255*dir_grad/np.max(dir_grad))\n",
    "    dir_binary = np.zeros_like(dir_grad)\n",
    "    dir_binary[(dir_grad >= dir_thresh[0]) & (dir_grad <= dir_thresh[1])] = 1\n",
    "    return dir_binary\n",
    "\n",
    "def s_select(img, thresh):\n",
    "    \"\"\" 1) Converts image to HLS color space\n",
    "        2) Apply a threshold to the S channel\n",
    "        3) Return a binary image of threshold result \"\"\"\n",
    "    \n",
    "    img = cv2.cvtColor(img,cv2.COLOR_BGR2HLS)\n",
    "    img_s = img[:,:,2]\n",
    "\n",
    "    sbinary = np.zeros_like(img_s)\n",
    "    sbinary[(img_s > thresh[0]) & (img_s <= thresh[1])] = 1\n",
    "    return sbinary\n",
    "\n",
    "def l_select(img, thresh):\n",
    "    \"\"\" 1) Converts image to HLS color space\n",
    "        2) Apply a threshold to the S channel\n",
    "        3) Return a binary image of threshold result \"\"\"\n",
    "    \n",
    "    img = cv2.cvtColor(img,cv2.COLOR_BGR2HLS)\n",
    "    img_l = img[:,:,1]\n",
    "\n",
    "    lbinary = np.zeros_like(img_l)\n",
    "    lbinary[(img_l > thresh[0]) & (img_l <= thresh[1])] = 1\n",
    "    return lbinary\n",
    "\n",
    "def canny(img, low_threshold, high_threshold):\n",
    "    \"\"\"Applies the Canny transform\"\"\"\n",
    "    return cv2.Canny(img, low_threshold, high_threshold)\n",
    "\n",
    "def gaussian_blur(img, kernel_size):\n",
    "    \"\"\"Applies a Gaussian Noise kernel\"\"\"\n",
    "    return cv2.GaussianBlur(img, (kernel_size, kernel_size), 0)\n",
    "\n",
    "def region_of_interest(img):\n",
    "    \"\"\"\n",
    "    Applies an image mask.\n",
    "    Only keeps the region of the image defined by the polygon\n",
    "    formed from `vertices`. The rest of the image is set to black.\n",
    "    \"\"\"\n",
    "    global terrain_type\n",
    "    \n",
    "    if terrain_type == 'h':\n",
    "        mask_top_y = 470\n",
    "        mask_bottom_y = 720\n",
    "        mask_top_left_x = 360\n",
    "#        mask_top_right_x = 680\n",
    "        mask_top_right_x = 800\n",
    "        mask_bottom_right_x = 1190\n",
    "#        mask_bottom_right_x = 1240\n",
    "        mask_bottom_left_x = 75\n",
    "    elif terrain_type == 'c':\n",
    "        mask_top_y = 470\n",
    "        mask_bottom_y = 720\n",
    "        mask_top_left_x = 580\n",
    "        mask_top_right_x = 750\n",
    "#        mask_bottom_right_x = 1150\n",
    "        mask_bottom_right_x = 1125\n",
    "        mask_bottom_left_x = 180\n",
    "    else:\n",
    "        mask_top_y = 440\n",
    "        mask_bottom_y = 720\n",
    "        mask_top_left_x = 530\n",
    "        mask_top_right_x = 740\n",
    "        mask_bottom_right_x = 1165\n",
    "        mask_bottom_left_x = 150\n",
    "        \n",
    "    \n",
    "    vertices = np.array([[(mask_top_left_x, mask_top_y), (mask_top_right_x, mask_top_y),  \\\n",
    "                          (mask_bottom_right_x, mask_bottom_y), (mask_bottom_left_x, mask_bottom_y)]],  \\\n",
    "                             dtype=np.int32) # vertices of the masked area\n",
    "\n",
    "    #defining a blank mask to start with\n",
    "    mask = np.zeros_like(img)\n",
    "\n",
    "    #defining a 3 channel or 1 channel color to fill the mask with depending on the input image\n",
    "    if len(img.shape) > 2:\n",
    "        channel_count = img.shape[2]  # i.e. 3 or 4 depending on your image\n",
    "        ignore_mask_color = (255,) * channel_count\n",
    "    else:\n",
    "        ignore_mask_color = 255\n",
    "\n",
    "    #filling pixels inside the polygon defined by \"vertices\" with the fill color\n",
    "    cv2.fillPoly(mask, vertices, ignore_mask_color)\n",
    "\n",
    "    #returning the image only where mask pixels are nonzero\n",
    "    masked_image = cv2.bitwise_and(img, mask)\n",
    "\n",
    "    return masked_image\n",
    "\n",
    "def calculate_curvature(y_eval, fit):\n",
    "    \"\"\"Calculates the radius of curvature of a given line\"\"\"\n",
    "    # y_eval : y-value where we want radius of curvature\n",
    "    # fit : left or right line fit\n",
    "    return ((1 + (2*fit[0]*y_eval + fit[1])**2)**1.5) / np.absolute(2*fit[0])\n",
    "\n",
    "def hough_lines(img, rho, theta, threshold, min_line_len, max_line_gap):\n",
    "    \"\"\"\n",
    "    Returns an image with hough lines drawn.\n",
    "    \"\"\"\n",
    "    lines = cv2.HoughLinesP(img, rho, theta, threshold, np.array([]), minLineLength=min_line_len, maxLineGap=max_line_gap)\n",
    "    \n",
    "    return lines\n",
    "\n",
    "def img_collage(marked_img):\n",
    "    \"\"\" Concatenate images into a single image\n",
    "        This helps view different perspectives of the image\n",
    "        at the same time helping with debugging\"\"\"\n",
    "    global c1, c2\n",
    "    c3 = np.concatenate((c1,c2),0)    \n",
    "    marked_img = cv2.resize(marked_img,(1280, 720))\n",
    "    c0 = cv2.copyMakeBorder(marked_img,0,10,0,10,cv2.BORDER_CONSTANT, value=[255,255,0])\n",
    "    ct = np.concatenate((c0,c3),1)\n",
    "    c = np.concatenate((ct,cb),0)\n",
    "    c = cv2.resize(c,(1280, 720))\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Function to extract a warped binary image of the lanes\n",
    "\n",
    "\"\"\" This function takes in a 3 channel RGB olor image and \n",
    "    returns a warped binary image with the lane markings only \"\"\"\n",
    "\n",
    "    # Undistort the image\n",
    "    # Create necessary color channel filters\n",
    "    # Create necessary sobel filters\n",
    "    # Create a combined filter that provides the best results\n",
    "    # Apply mask to pick up lane pixels only\n",
    "    # Warp the image\n",
    "\n",
    "def get_binary_warp(image):\n",
    "\n",
    "    # Undistort the camera image\n",
    "    image = undistort(image)\n",
    "    \n",
    "    lighting_condition = check_lighting(image)\n",
    "    if lighting_condition == \"Poor\":\n",
    "        image = switch_on_headlights(image)\n",
    "\n",
    "    # Get image filtered by S channel\n",
    "#    s_chn_img = s_select(image, thresh=(90,255))\n",
    "    s_chn_img = s_select(image, thresh=(100,255))\n",
    "    \n",
    "    # Get image filtered by L channel\n",
    "    l_chn_img = l_select(image, thresh=(50,255))\n",
    "    \n",
    "    # Grayscale the image.  Filter out low intensity pixels to remove black patches / lines from image.\n",
    "    gimg = grayscale(image)\n",
    "    gray_img = grayscale(image)\n",
    "    gray_img[gray_img < 200] = 0\n",
    "\n",
    "    \n",
    "    # Get image filtered by sobel x\n",
    "    sobelx_img = gradient(gray_img, orient='x', sobel_kernel=9, thresh=(20,100))\n",
    "    \n",
    "    # Get image filtered by sobel y\n",
    "    sobely_img = gradient(gray_img, orient='y', sobel_kernel=9, thresh=(20,100))\n",
    "    \n",
    "    # Get image filtered by sobel magnitude\n",
    "#    sobel_mag = gradient_mag(gray_img, sobel_kernel=3, mag_thresh=(20,100))\n",
    "    \n",
    "    # Get image filtered by sobel gradient direction\n",
    "#    sobel_dir = gradient_dir(gray_img, sobel_kernel=31, dir_thresh=(0.7,1.3))\n",
    "    \n",
    "    # Stack each channel to view their individual contributions in green and blue respectively\n",
    "    # This returns a stack of the two binary images, whose components you can see as different colors\n",
    "    #color_binary = np.dstack(( np.zeros_like(sobelx_img), sobelx_img, s_chn_img))\n",
    "    \n",
    "    # Combine the two binary thresholds\n",
    "#    combined_img = np.zeros_like(sobelx_img)\n",
    "#    combined_img[((s_chn_img == 1) | ((sobelx_img == 1) & (sobely_img ==1))) & ((l_chn_img == 1))] = 1\n",
    "\n",
    "    # Pick up pixels that have high S and L channel values.  This will help address shadow conditions\n",
    "    combined_img2 = np.zeros_like(sobelx_img)\n",
    "    combined_img2[((s_chn_img == 1) & (l_chn_img == 1))] = 1\n",
    "    \n",
    "    # Pick up pixels that have high sobel x and y values.  This will help pick only strong pixels.\n",
    "    combined_img3 = np.zeros_like(sobelx_img)\n",
    "    combined_img3[((sobelx_img == 1) & (sobely_img ==1))] = 1\n",
    "\n",
    "    # Combine the two binary images.  This image :\n",
    "    # 1. Will capture lanes under shadows as well\n",
    "    # 2. Will pick up only strong pixels that have high values both in x and y\n",
    "    combined_img4 = np.zeros_like(sobelx_img)\n",
    "    combined_img4[((combined_img2 == 1) | (combined_img3 ==1))] = 1\n",
    "    \n",
    "\n",
    "    # Apply mask for lane region only to filter out non-lane pixels.\n",
    "    masked_image = region_of_interest(combined_img4) \n",
    "    \n",
    "    # Warp the image to provide a top down perspective view\n",
    "    warped_image = cv2.warpPerspective(masked_image, perspective_M, gray_img.shape[::-1], flags=cv2.INTER_LINEAR)\n",
    "#    showGrayImg(warped_image)\n",
    "\n",
    "    \"\"\"\n",
    "    # Plot the images for review\n",
    "    f = plt.figure(figsize=(60, 33))\n",
    "    ax1 = plt.subplot(3, 4, 1)\n",
    "    ax1.imshow(image)\n",
    "    ax1.set_title('Original Image', fontsize=30)\n",
    "    ax2 = plt.subplot(3, 4, 2)\n",
    "    ax2.imshow(s_chn_img, cmap='gray')\n",
    "    ax2.set_title('S Channel', fontsize=30)\n",
    "    ax3 = plt.subplot(3, 4, 3)\n",
    "    ax3.imshow(l_chn_img, cmap='gray')\n",
    "    ax3.set_title('L Channel', fontsize=30)\n",
    "    ax4 = plt.subplot(3, 4, 4)\n",
    "    ax4.imshow(combined_img2, cmap=\"gray\")\n",
    "    ax4.set_title('S & L combined Img2', fontsize=30)\n",
    "    ax5 = plt.subplot(3, 4, 5)\n",
    "    ax5.imshow(gimg, cmap=\"gray\")\n",
    "    ax5.set_title('Gray Img', fontsize=30)\n",
    "    ax6 = plt.subplot(3, 4, 6)\n",
    "    ax6.imshow(gray_img, cmap=\"gray\")\n",
    "    ax6.set_title('Filtered Gray Image', fontsize=30)\n",
    "    ax7 = plt.subplot(3, 4, 7)\n",
    "    ax7.imshow(sobelx_img, cmap=\"gray\")\n",
    "    ax7.set_title('Gradient x', fontsize=30)\n",
    "    ax8 = plt.subplot(3, 4, 8)\n",
    "    ax8.imshow(sobely_img, cmap=\"gray\")\n",
    "    ax8.set_title('Gradient y', fontsize=30)\n",
    "    ax9 = plt.subplot(3, 4, 9)\n",
    "    ax9.imshow(combined_img3, cmap=\"gray\")\n",
    "    ax9.set_title('x & y Gradient combined Img3', fontsize=30)\n",
    "    ax10 = plt.subplot(3, 4, 10)\n",
    "    ax10.imshow(combined_img4, cmap=\"gray\")\n",
    "    ax10.set_title('Combined2 + Combined 3 = Combined Img4', fontsize=30)\n",
    "    ax11 = plt.subplot(3, 4, 11)\n",
    "    ax11.imshow(masked_image, cmap=\"gray\")\n",
    "    ax11.set_title('Masked View', fontsize=30)\n",
    "    ax12 = plt.subplot(3, 4, 12)\n",
    "    ax12.imshow(warped_image, cmap=\"gray\")\n",
    "    ax12.set_title('Warped Image', fontsize=30)\n",
    "#    plt.subplots_adjust(left=0., right=1, top=0.5, bottom=0.)\n",
    "    f.savefig('BinaryThreshold.png')\n",
    "    plt.show()\n",
    "    \"\"\"\n",
    "    \n",
    "    # Combine images to create a collage of different perspectives\n",
    "    # This makes it easy to debug as many perspective are shown at the same time\n",
    "    c4 = cv2.resize(combined_img4,(640, 360))\n",
    "    c4 = np.dstack((c4, c4, c4))*255\n",
    "    c4 = cv2.copyMakeBorder(c4,0,0,0,5,cv2.BORDER_CONSTANT, value=[255,255,0])\n",
    "    cv2.putText(c4,\"Filtered Binary Image\", (150,30), cv2.FONT_HERSHEY_PLAIN, 2, (255,255,255))\n",
    "    if lighting_condition == \"Poor\":\n",
    "        cv2.putText(c4,\"Head Lights ON\", (150,330), cv2.FONT_HERSHEY_PLAIN, 2, (255,255,0))\n",
    "    else:\n",
    "        cv2.putText(c4,\"Head Lights OFF\", (150,330), cv2.FONT_HERSHEY_PLAIN, 2, (100,100,100))\n",
    "#    print(\"c4 shape = \", c4.shape)\n",
    "    c5 = cv2.resize(masked_image,(640, 360))\n",
    "    c5 = np.dstack((c5, c5, c5))*255\n",
    "    c5 = cv2.copyMakeBorder(c5,0,0,0,5,cv2.BORDER_CONSTANT, value=[255,255,0])\n",
    "    cv2.putText(c5,\"Masked Binary Image\", (150,30), cv2.FONT_HERSHEY_PLAIN, 2, (255,255,255))\n",
    "#    print(\"c5 shape = \", c5.shape)\n",
    "    c6 = cv2.resize(warped_image,(640, 360))\n",
    "    c6 = np.dstack((c6, c6, c6))*255\n",
    "#    c6 = cv2.copyMakeBorder(c6,0,0,0,0,cv2.BORDER_CONSTANT, value=[255,255,0])\n",
    "    cv2.putText(c6,\"Warped\", (180,30), cv2.FONT_HERSHEY_PLAIN, 2, (255,255,255))\n",
    "    cv2.putText(c6,\"Image\", (180,60), cv2.FONT_HERSHEY_PLAIN, 2, (255,255,255))\n",
    "#    print(\"c6 shape = \", c6.shape)\n",
    "    global cb\n",
    "    cb = np.concatenate((c4, c5, c6),1)\n",
    "\n",
    "    return warped_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### This function will locate a fresh set of lane lines from a given binary warped image\n",
    "\"\"\"  This function called \n",
    "     1. During the start of the pipeline to find the lanes\n",
    "     2. Whenever continous bad lanes are detected - this will help as a reset\"\"\"\n",
    "\n",
    "def find_fresh_lanes(binary_warped):\n",
    "    \n",
    "#    print(\"Inside reset\")\n",
    "    drive_log.write(\"@ \" + str(len(detected_lines)) + \" - Inside reset\\n\")\n",
    "    \n",
    "    # Take a histogram of the bottom half of the image\n",
    "    histogram = np.sum(binary_warped[binary_warped.shape[0]/2:,:], axis=0)\n",
    "#    plt.plot(histogram)\n",
    "#    plt.show()\n",
    "    \n",
    "\n",
    "    # Find the peak of the left and right halves of the histogram\n",
    "    # These will be the starting point for the left and right lines\n",
    "    midpoint = np.int(histogram.shape[0]/2)\n",
    "    leftx_base = np.argmax(histogram[:midpoint])\n",
    "    rightx_base = np.argmax(histogram[midpoint:]) + midpoint\n",
    "#    print(leftx_base, rightx_base)\n",
    "\n",
    "    # Choose the number of sliding windows\n",
    "    nwindows = 9\n",
    "    # Set height of windows\n",
    "    window_height = np.int(binary_warped.shape[0]/nwindows)\n",
    "    \n",
    "    # Identify the x and y positions of all nonzero pixels in the image\n",
    "    nonzero = binary_warped.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "    \n",
    "    # Current positions to be updated for each window\n",
    "    leftx_current = leftx_base\n",
    "    rightx_current = rightx_base\n",
    "    \n",
    "    # Set the width of the windows +/- margin\n",
    "    margin = 100\n",
    "    # Set minimum number of pixels found to recenter window\n",
    "    minpix = 20\n",
    "    \n",
    "    # Create empty lists to receive left and right lane pixel indices\n",
    "    left_lane_inds = []\n",
    "    right_lane_inds = []\n",
    "    \n",
    "    # Create an output image to draw on and  visualize the result\n",
    "    out_img = np.dstack((binary_warped, binary_warped, binary_warped))*255\n",
    "    \n",
    "    bad_left_box = 0\n",
    "    bad_right_box = 0\n",
    "    \n",
    "    # Step through the windows one by one\n",
    "    for window in range(nwindows):\n",
    "        # Identify window boundaries in x and y (and right and left)\n",
    "        win_y_low = binary_warped.shape[0] - (window+1)*window_height\n",
    "        win_y_high = binary_warped.shape[0] - window*window_height\n",
    "        win_xleft_low = leftx_current - margin\n",
    "        win_xleft_high = leftx_current + margin\n",
    "        win_xright_low = rightx_current - margin\n",
    "        win_xright_high = rightx_current + margin\n",
    "        # Draw the windows on the visualization image\n",
    "        cv2.rectangle(out_img,(win_xleft_low,win_y_low),(win_xleft_high,win_y_high),(0,255,0), 3) \n",
    "        cv2.rectangle(out_img,(win_xright_low,win_y_low),(win_xright_high,win_y_high),(0,255,0), 3) \n",
    "        \n",
    "        # Identify the nonzero pixels in x and y within the window\n",
    "        if bad_left_box < 3:\n",
    "            good_left_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & (nonzerox >= win_xleft_low) & (nonzerox < win_xleft_high)).nonzero()[0]\n",
    "            left_lane_inds.append(good_left_inds)\n",
    "            if len(good_left_inds) == 0:\n",
    "                bad_left_box += 1\n",
    "\n",
    "        if bad_right_box < 3:\n",
    "            good_right_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & (nonzerox >= win_xright_low) & (nonzerox < win_xright_high)).nonzero()[0]\n",
    "            right_lane_inds.append(good_right_inds)\n",
    "            if len(good_right_inds) == 0:\n",
    "                bad_right_box += 1\n",
    "\n",
    "        # If you found > minpix pixels, recenter next window on their mean position\n",
    "        if len(good_left_inds) > minpix:\n",
    "            leftx_current = np.int(np.mean(nonzerox[good_left_inds]))\n",
    "        if len(good_right_inds) > minpix:        \n",
    "            rightx_current = np.int(np.mean(nonzerox[good_right_inds]))\n",
    "        \n",
    "    # Concatenate the arrays of indices\n",
    "    left_lane_inds = np.concatenate(left_lane_inds)\n",
    "    right_lane_inds = np.concatenate(right_lane_inds)\n",
    "\n",
    "    # Combine images to create a collage of different perspectives\n",
    "    # This makes it easy to debug as many perspective are shown at the same time\n",
    "    global c2\n",
    "    c2 = cv2.resize(out_img,(640, 360))\n",
    "    c2 = cv2.copyMakeBorder(c2,0,10,0,0,cv2.BORDER_CONSTANT, value=[255,255,0])\n",
    "#    showImg(c2)\n",
    " \n",
    "    # Reset the flag to indicate that fresh detection has been done\n",
    "    global reset_lanes\n",
    "    reset_lanes = False\n",
    "    \n",
    "#    showImg(out_img)\n",
    "\n",
    "    return leftx_base, rightx_base, left_lane_inds, right_lane_inds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Perform sanity checks.\n",
    "\"\"\" 1. Sanity check the left and right lanes from the current detection.\n",
    "    2. Sanity check the lanes against previous good values for continuity \"\"\"\n",
    "\n",
    "def sanity_check_lanes(l):\n",
    "    \n",
    "    good_lane = True\n",
    "#    print(\"inside sanity\")\n",
    "    \n",
    "    while True:\n",
    "\n",
    "        # Low Pass Filter - Do the left and rigth lanes have enough pixels\n",
    "        if len(l.leftx) < 10 or len(l.rightx) < 10:\n",
    "            good_lane = False\n",
    "#            print(\"@\", len(detected_lines),  \" > low pass failure : \", len(l.leftx), len(l.rightx) )\n",
    "            drive_log.write(\"@ \" + str(len(detected_lines)) + \" > low pass failure : \" \n",
    "                            + str(len(l.leftx)) + \" \" + str(len(l.rightx)) + \"\\n\")\n",
    "            break\n",
    "\n",
    "        # Is the lane width at base reasonable (around 3.7 m)\n",
    "        if not(3 < l.lane_width < 4.5):\n",
    "            good_lane = False\n",
    "#            print(\"@\", len(detected_lines),  \" > lane width failure : \", l.lane_width)\n",
    "            drive_log.write(\"@ \" + str(len(detected_lines)) + \" > base lane width failure : \" \n",
    "                            + str(l.lane_width) + \"\\n\")\n",
    "            break\n",
    "\n",
    "        # Is the lane width at top reasonable (around 3.7 m)\n",
    "        if not(3 < l.top_lane_width < 7):\n",
    "            good_lane = False\n",
    "#            print(\"@\", len(detected_lines),  \" > lane width failure : \", l.lane_width)\n",
    "            drive_log.write(\"@ \" + str(len(detected_lines)) + \" > top lane width failure : \" \n",
    "                            + str(l.top_lane_width) + \"\\n\")\n",
    "            break\n",
    "        \n",
    "        # Are the lane curvatures reasonable\n",
    "        if l.left_curverad > 30000 or l.right_curverad > 30000:\n",
    "            good_lane = False\n",
    "#            print(\"@\", len(detected_lines),  \" > curve check failure : \", l.left_curverad, l.right_curverad)\n",
    "            drive_log.write(\"@ \" + str(len(detected_lines)) + \" > curve check failure : \" \n",
    "                            + str(l.left_curverad) + \" \" + str(l.right_curverad) + \"\\n\")\n",
    "            break\n",
    "\n",
    "        # Do the left and right lanes have similar curvature\n",
    "        if abs(l.left_curverad - l.right_curverad) > 20000:\n",
    "            good_lane = False\n",
    "#            print(\"@\", len(detected_lines),  \" > left and right curves similarity failure : \", l.left_curverad, l.right_curverad)\n",
    "            drive_log.write(\"@ \" + str(len(detected_lines)) + \" > left and right curves similarity failure :\" \n",
    "                            + str(l.left_curverad) + \" \" + str(l.right_curverad) + \"\\n\")\n",
    "            break\n",
    "            \n",
    "        # Are the lanes parallel (co-eff check)\n",
    "#        if abs(l.left_fit[0] - l.right_fit[0]) > 0.0008:\n",
    "        if abs(l.left_fit[0] - l.right_fit[0]) > 0.0025:\n",
    "            good_lane = False\n",
    "#            print(\"@\", len(detected_lines),  \" > 2nd order coefficient similarity failure : \", l.left_fit[0], l.right_fit[0])\n",
    "            drive_log.write(\"@ \" + str(len(detected_lines)) + \" > 2nd order coefficient similarity failure :\" \n",
    "                            + str(l.left_fit[0]) + \" \" + str(l.right_fit[0]) + \"\\n\")\n",
    "            break\n",
    "            \n",
    "        if abs(l.left_fit[1] - l.right_fit[1]) > 1.3:\n",
    "            good_lane = False\n",
    "#            print(\"@\", len(detected_lines),  \" > 1st order coefficient similarity failure : \", l.left_fit[1], l.right_fit[1])\n",
    "            drive_log.write(\"@ \" + str(len(detected_lines)) + \" > 1st order coefficient similarity failure :\" \n",
    "                            + str(l.left_fit[1]) + \" \" + str(l.right_fit[1]) + \"\\n\")\n",
    "            break\n",
    "        \n",
    "        if not l.fresh_lane:\n",
    "            \n",
    "            # Is the left lane position close to value from prev frame\n",
    "                if abs(l.leftx_base - detected_lines[-1].leftx_base) > 100:\n",
    "                    good_lane = False\n",
    "#                    print(\"@\", len(detected_lines),  \" > Continuity check failed - left lane base offset high : \", \n",
    "#                          l.leftx_base, detected_lines[-1].leftx_base)\n",
    "                    drive_log.write(\"@ \" + str(len(detected_lines)) + \" > Continuity check failed - left lane base offset high : \" \n",
    "                                    + str(l.leftx_base) + \" \" + str(detected_lines[-1].leftx_base) + \"\\n\")\n",
    "                    break            \n",
    "\n",
    "            # Is the right lane position close to value from prev frame\n",
    "                if abs(l.rightx_base - detected_lines[-1].rightx_base) > 50:\n",
    "                    good_lane = False\n",
    "#                    print(\"@\", len(detected_lines),  \" > Continuity check failed - left lane base offset high : \", \n",
    "#                          l.leftx_base, detected_lines[-1].leftx_base)\n",
    "                    drive_log.write(\"@ \" + str(len(detected_lines)) + \" > Continuity check failed - right lane base offset high : \" \n",
    "                                    + str(l.rightx_base) + \" \" + str(detected_lines[-1].rightx_base) + \"\\n\")\n",
    "                    break    \n",
    "        \n",
    "            # Is the lane offset marginal from prev frame\n",
    "                if abs(l.center_offset_m - detected_lines[-1].center_offset_m) > 0.5:\n",
    "                    good_lane = False\n",
    "#                    print(\"@\", len(detected_lines),  \" > Continuity check failed - lane offset variation high : \", \n",
    "#                          l.center_offset_m, detected_lines[-1].center_offset_m)\n",
    "                    drive_log.write(\"@ \" + str(len(detected_lines)) + \" > Continuity check failed - lane offset variation high : \" \n",
    "                                    + str(l.center_offset_m) + \" \" + str(detected_lines[-1].center_offset_m) + \"\\n\")\n",
    "                    break\n",
    "\n",
    "            # Is the left curvature offset reasonable from prev frame\n",
    "#                if abs(l.left_curverad - detected_lines[-1].left_curverad) > abs(3.0 * l.left_curverad):\n",
    "                if 4.0 < (l.left_curverad / detected_lines[-1].left_curverad) < 0.25:\n",
    "                    good_lane = False\n",
    "#                    print(\"@\", len(detected_lines),  \" > Continuity check failed - left curve variation high : \", \n",
    "#                          l.left_curverad, detected_lines[-1].left_curverad)\n",
    "                    drive_log.write(\"@ \" + str(len(detected_lines)) + \n",
    "                                    \" > Continuity check failed - left curve variation high : \" \n",
    "                                    + str(l.left_curverad) + \" \" + str(detected_lines[-1].left_curverad) + \"\\n\")\n",
    "                    break\n",
    "            \n",
    "            # Is the right curvature offset reasonable from prev frame\n",
    "#                if abs(l.right_curverad - detected_lines[-1].right_curverad) > abs(3.0 * l.right_curverad):\n",
    "                if 4.0 < (l.right_curverad / detected_lines[-1].right_curverad) < 0.25:\n",
    "                    good_lane = False\n",
    "#                    print(\"@\", len(detected_lines),  \" > Continuity check failed - right curve variation high : \", \n",
    "#                          l.right_curverad, detected_lines[-1].right_curverad)\n",
    "                    drive_log.write(\"@ \" + str(len(detected_lines)) + \n",
    "                                    \" > Continuity check failed - right curve variation high : \" \n",
    "                                    + str(l.right_curverad) + \" \" + str(detected_lines[-1].right_curverad) + \"\\n\")\n",
    "                    break\n",
    "\n",
    "            # Is the co-eff offset reasonable from prev frame\n",
    "            \n",
    "        break\n",
    "            \n",
    "#    print(\"lane quality = \", good_lane)\n",
    "    return good_lane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Function to extract the left and right line fits from a given warped binary image of the lanes\n",
    "\n",
    "\"\"\" This function takes in a warped binary image of the lanes and \n",
    "    returns the left and right lane fits \"\"\"\n",
    "\n",
    "def get_line_fits(binary_warped):\n",
    "    \n",
    "#    print(\"inside get line fit\")\n",
    "    \n",
    "#    showGrayImg(binary_warped)\n",
    "    global reset_lanes\n",
    "#    print('reset status', reset_lanes)\n",
    "\n",
    "    # Identify indices of non-zero pixels from the binary image\n",
    "    nonzero = binary_warped.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "\n",
    "    # Create a new Line object to store the lane parameters\n",
    "    l = Line()\n",
    "    \n",
    "    if reset_lanes :\n",
    "        # Fresh lanes being built\n",
    "        leftx_base, rightx_base, left_lane_inds, right_lane_inds = find_fresh_lanes(binary_warped)\n",
    "        l.fresh_lane = True\n",
    "    else:\n",
    "        # Using coeff from last good detection for plotting new lanes\n",
    "        l.fresh_lane = False\n",
    "        margin = 100\n",
    "        left_fit = detected_lines[-1].left_fit\n",
    "        right_fit = detected_lines[-1].right_fit\n",
    "        left_lane_inds = ((nonzerox > (left_fit[0]*(nonzeroy**2) + left_fit[1]*nonzeroy + left_fit[2] - margin)) & (nonzerox < (left_fit[0]*(nonzeroy**2) + left_fit[1]*nonzeroy + left_fit[2] + margin))) \n",
    "        right_lane_inds = ((nonzerox > (right_fit[0]*(nonzeroy**2) + right_fit[1]*nonzeroy + right_fit[2] - margin)) & (nonzerox < (right_fit[0]*(nonzeroy**2) + right_fit[1]*nonzeroy + right_fit[2] + margin)))\n",
    "        y_base = binary_warped.shape[0]\n",
    "        leftx_base = left_fit[0]*(y_base**2) + left_fit[1]*y_base + left_fit[2]\n",
    "        rightx_base = right_fit[0]*(y_base**2) + right_fit[1]*y_base + right_fit[2]\n",
    "\n",
    "#        print(leftx_base, rightx_base)\n",
    "\n",
    "    # Extract left and right line pixel positions\n",
    "    leftx = nonzerox[left_lane_inds]\n",
    "    lefty = nonzeroy[left_lane_inds] \n",
    "    rightx = nonzerox[right_lane_inds]\n",
    "    righty = nonzeroy[right_lane_inds] \n",
    "\n",
    "\n",
    "    # Fit a second order polynomial to each lane\n",
    "    if len(leftx) == 0 or len(lefty) == 0:\n",
    "        leftx = detected_lines[-1].leftx\n",
    "        lefty = detected_lines[-1].lefty\n",
    "    if len(rightx) == 0 or len(righty) == 0:\n",
    "        rightx = detected_lines[-1].rightx\n",
    "        righty = detected_lines[-1].righty\n",
    "\n",
    "    left_fit = np.polyfit(lefty, leftx, 2)\n",
    "    right_fit = np.polyfit(righty, rightx, 2)\n",
    "\n",
    "\n",
    "    # Create an output image to draw on and  visualize the result\n",
    "    out_img = np.dstack((binary_warped, binary_warped, binary_warped))*255\n",
    "    out_img[nonzeroy[left_lane_inds], nonzerox[left_lane_inds]] = [255, 0, 0]\n",
    "    out_img[nonzeroy[right_lane_inds], nonzerox[right_lane_inds]] = [0, 0, 255]\n",
    "#    showImg(out_img)\n",
    "\n",
    "\n",
    "    # Calculate radius of curvature\n",
    "    global baseline_lane_width\n",
    "    ploty = np.linspace(0, binary_warped.shape[0]-1, binary_warped.shape[0] )\n",
    "    y_eval = np.max(ploty) / 2  # y-value where we want radius of curvature\n",
    "    ym_per_pix = 30/720 # meters per pixel in y dimension  -->  Manual observation\n",
    "    xm_per_pix = 3.7/baseline_lane_width # meters per pixel in x dimension\n",
    "#    xm_per_pix = 3.7/700 # meters per pixel in x dimension\n",
    "#    ym_per_pix = 3/7 # meters per pixel in y dimension  -->  Manual observation\n",
    "\n",
    "\n",
    "    # Calculate curvature in pixels\n",
    "    left_curverad = calculate_curvature(y_eval, left_fit)\n",
    "    right_curverad = calculate_curvature(y_eval, right_fit)\n",
    "#    print(\"Curvature left = \", left_curverad, \"px  right = \", right_curverad, \"px\")\n",
    "    \n",
    "    # Fit new polynomials to x,y in world space to calculate curvature in meters\n",
    "    left_fit_cr = np.polyfit(lefty*ym_per_pix, leftx*xm_per_pix, 2)\n",
    "    right_fit_cr = np.polyfit(righty*ym_per_pix, rightx*xm_per_pix, 2)\n",
    "    \n",
    "    # Calculate the new radii of curvature\n",
    "    left_curverad = ((1 + (2*left_fit_cr[0]*y_eval*ym_per_pix + left_fit_cr[1])**2)**1.5) / np.absolute(2*left_fit_cr[0])\n",
    "    right_curverad = ((1 + (2*right_fit_cr[0]*y_eval*ym_per_pix + right_fit_cr[1])**2)**1.5) / np.absolute(2*right_fit_cr[0])\n",
    "     \n",
    "    # Determine lane width and offset\n",
    "    lane_width = (rightx_base - leftx_base) * xm_per_pix\n",
    "    leftx_top = left_fit[2]\n",
    "    rightx_top = right_fit[2]\n",
    "    top_lane_width = (rightx_top - leftx_top) * xm_per_pix\n",
    "    lane_center = int(leftx_base + ((rightx_base - leftx_base) / 2))\n",
    "    img_center = np.int(binary_warped.shape[1]/2)\n",
    "    center_offset_px = img_center - lane_center\n",
    "    center_offset_m = round((center_offset_px * xm_per_pix), 2)\n",
    "\n",
    "    # Store detect lane parametes in the line object\n",
    "    l.leftx_base = leftx_base\n",
    "    l.rightx_base = rightx_base\n",
    "    l.leftx = leftx\n",
    "    l.lefty = lefty\n",
    "    l.rightx = rightx\n",
    "    l.righty = righty\n",
    "    l.left_fit = left_fit\n",
    "    l.right_fit = right_fit\n",
    "    l.left_curverad = left_curverad\n",
    "    l.right_curverad = right_curverad\n",
    "    l.center_offset_m = center_offset_m\n",
    "    l.lane_width = lane_width\n",
    "    l.top_lane_width = top_lane_width\n",
    "    l.smooth_leftx = leftx\n",
    "    l.smooth_rightx = rightx\n",
    "    l.smooth_left_fit = left_fit[:]\n",
    "    l.smooth_right_fit = right_fit[:]\n",
    "\n",
    "    # Sanity check the identified lanes.\n",
    "    if len(detected_lines) == 0:\n",
    "        # Handle first clip of the pipeline\n",
    "#        print(\"inside first frame\")\n",
    "        l.good_lane = True\n",
    "        l.continous_bad_lanes = 0\n",
    "    else:\n",
    "        l.continous_bad_lanes = detected_lines[-1].continous_bad_lanes\n",
    "        l.good_lane = sanity_check_lanes(l)        \n",
    "\n",
    "    # Create an image section that will show lane parameters\n",
    "    # This makes it easy to debug\n",
    "    global c1\n",
    "    c1 = np.zeros_like(out_img)\n",
    "    c1 = cv2.resize(c1,(640, 360))\n",
    "    c1.fill(255)\n",
    "\n",
    "    cv2.putText(c1,\"Frame Metrics - \" + str(len(detected_lines)), (200,30), cv2.FONT_HERSHEY_PLAIN, 2, (0,0,255))\n",
    "    cv2.putText(c1,\"lx_base = \" + str(int(leftx_base)), (10,60), cv2.FONT_HERSHEY_PLAIN, 2, (0,0,255))\n",
    "    cv2.putText(c1,\"rx_base = \" + str(int(rightx_base)), (10,90), cv2.FONT_HERSHEY_PLAIN, 2, (0,0,255))\n",
    "    cv2.putText(c1,\"bln_width= \" + str(round(lane_width,2)), (10,120), cv2.FONT_HERSHEY_PLAIN, 2, (0,0,255))\n",
    "    cv2.putText(c1,\"tln_width= \" + str(round(top_lane_width,2)), (10,150), cv2.FONT_HERSHEY_PLAIN, 2, (0,0,255))\n",
    "    cv2.putText(c1,\"lfit2 = \" + str(round(left_fit[2],2)), (10,180), cv2.FONT_HERSHEY_PLAIN, 2, (0,0,255))\n",
    "    cv2.putText(c1,\"rfit2 = \" + str(round(right_fit[2],2)), (10,210), cv2.FONT_HERSHEY_PLAIN, 2, (0,0,255))\n",
    "    cv2.putText(c1,\"lfit1 = \" + str(round(left_fit[1],3)), (10,240), cv2.FONT_HERSHEY_PLAIN, 2, (0,0,255))\n",
    "    cv2.putText(c1,\"rfit1 = \" + str(round(right_fit[1],3)), (10,270), cv2.FONT_HERSHEY_PLAIN, 2, (0,0,255))\n",
    "    cv2.putText(c1,\"lfit0 = \" + str(round(left_fit[0],6)), (10,300), cv2.FONT_HERSHEY_PLAIN, 2, (0,0,255))\n",
    "    cv2.putText(c1,\"rfit0 = \" + str(round(right_fit[0],6)), (10,330), cv2.FONT_HERSHEY_PLAIN, 2, (0,0,255))\n",
    "    cv2.putText(c1,\"Fresh Lane = \" + str(l.fresh_lane), (295,60), cv2.FONT_HERSHEY_PLAIN, 2, (0,0,255))\n",
    "    cv2.putText(c1,\"Good Lane = \" + str(l.good_lane), (285,90), cv2.FONT_HERSHEY_PLAIN, 2, (0,0,255))\n",
    "    cv2.putText(c1,\"Bad Lns = \" + str(l.continous_bad_lanes), (285,120), cv2.FONT_HERSHEY_PLAIN, 2, (0,0,255))\n",
    "    cv2.putText(c1,\"Left Curve = \" + str(int(l.left_curverad)), (285,150), cv2.FONT_HERSHEY_PLAIN, 2, (0,0,255))\n",
    "    cv2.putText(c1,\"Right Curve = \" + str(int(l.right_curverad)), (285,180), cv2.FONT_HERSHEY_PLAIN, 2, (0,0,255))\n",
    "\n",
    "    # If the current lane detection is bad, use the previous good values to draw current lane\n",
    "    if l.good_lane:\n",
    "        l.continous_bad_lanes = 0\n",
    "        i = 1\n",
    "        # Smoothen the fit over the last n good detections\n",
    "        for line in detected_lines[::-1]:\n",
    "            l.smooth_leftx = l.smooth_leftx + l.leftx\n",
    "            l.smooth_rightx = l.smooth_rightx + l.rightx\n",
    "            l.smooth_left_fit = l.smooth_left_fit + line.left_fit\n",
    "            l.smooth_right_fit = l.smooth_right_fit + line.right_fit\n",
    "            i += 1\n",
    "            if i >= 5:\n",
    "                 break\n",
    "        l.smooth_leftx = l.smooth_leftx / i\n",
    "        l.smooth_rightx = l.smooth_rightx / i\n",
    "        l.smooth_left_fit = (l.smooth_left_fit) / i\n",
    "        l.smooth_right_fit = (l.smooth_right_fit) / i\n",
    "    else:\n",
    "        l.continous_bad_lanes += 1\n",
    "        if l.continous_bad_lanes > 3: \n",
    "            global reset_lanes\n",
    "            reset_lanes = True\n",
    "        l.leftx_base = detected_lines[-1].leftx_base\n",
    "        l.rightx_base = detected_lines[-1].rightx_base\n",
    "        l.leftx = detected_lines[-1].leftx\n",
    "        l.lefty = detected_lines[-1].lefty\n",
    "        l.rightx = detected_lines[-1].rightx\n",
    "        l.righty = detected_lines[-1].righty\n",
    "        l.left_fit = detected_lines[-1].left_fit\n",
    "        l.right_fit = detected_lines[-1].right_fit\n",
    "        l.left_curverad = detected_lines[-1].left_curverad\n",
    "        l.right_curverad = detected_lines[-1].right_curverad\n",
    "        l.center_offset_m = detected_lines[-1].center_offset_m\n",
    "        l.lane_width = detected_lines[-1].lane_width\n",
    "        l.top_lane_width = detected_lines[-1].top_lane_width\n",
    "        l.smooth_leftx = detected_lines[-1].leftx\n",
    "        l.smooth_rightx = detected_lines[-1].rightx\n",
    "        l.smooth_left_fit = detected_lines[-1].smooth_left_fit\n",
    "        l.smooth_right_fit = detected_lines[-1].smooth_right_fit\n",
    "    \n",
    "    # Add the current lane lines to the detected_lines list\n",
    "    detected_lines.append(l)\n",
    "\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Lane Marker Pipeline\n",
    "\"\"\" This function acts as a pipeline that takes a 3 channel RGB color image as input\n",
    "    and returns the same image with lanes marked in it \"\"\"\n",
    "\n",
    "def lane_marker_pipeline(image):\n",
    "\n",
    "    # Get the binary warped image of the input image\n",
    "    binary_warped = get_binary_warp(image)\n",
    "    \n",
    "    # Identify the left and right fits from the binary warped image\n",
    "    l = get_line_fits(binary_warped)\n",
    "    \n",
    "    # Generate x and y values for plotting\n",
    "    ploty = np.linspace(0, binary_warped.shape[0]-1, binary_warped.shape[0] )\n",
    "    left_fitx = l.smooth_left_fit[0]*ploty**2 + l.smooth_left_fit[1]*ploty + l.smooth_left_fit[2]\n",
    "    right_fitx = l.smooth_right_fit[0]*ploty**2 + l.smooth_right_fit[1]*ploty + l.smooth_right_fit[2]\n",
    "\n",
    "    # Create an output image to draw on and  visualize the result\n",
    "    out_img = np.dstack((binary_warped, binary_warped, binary_warped))*255\n",
    "    window_img = np.zeros_like(out_img)\n",
    "    \n",
    "    # Generate a polygon to illustrate the search window area\n",
    "    # And recast the x and y points into usable format for cv2.fillPoly()\n",
    "    line_width = 15\n",
    " \n",
    "    left_line_window1 = np.array([np.transpose(np.vstack([left_fitx - line_width, ploty]))])\n",
    "    left_line_window2 = np.array([np.flipud(np.transpose(np.vstack([left_fitx + line_width, ploty])))])\n",
    "    left_line_pts = np.hstack((left_line_window1, left_line_window2))\n",
    "    right_line_window1 = np.array([np.transpose(np.vstack([right_fitx - line_width, ploty]))])\n",
    "    right_line_window2 = np.array([np.flipud(np.transpose(np.vstack([right_fitx + line_width, ploty])))])\n",
    "    right_line_pts = np.hstack((right_line_window1, right_line_window2))\n",
    "\n",
    "    # Draw the lane onto the warped blank image\n",
    "    cv2.fillPoly(window_img, np.int_([left_line_pts]), (255,0, 0))\n",
    "    cv2.fillPoly(window_img, np.int_([right_line_pts]), (0,0, 255))\n",
    "    result = cv2.addWeighted(out_img, 0.5, window_img, 1.0, 0)\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    # Draw images\n",
    "    f = plt.figure(figsize=(30, 18))\n",
    "   \n",
    "    ax1 = plt.subplot(1, 3, 1)\n",
    "    ax1.imshow(binary_warped, cmap='gray')\n",
    "    ax1.set_title('Input Image', fontsize=10)\n",
    "    \n",
    "    ax2 = plt.subplot(1, 3, 2)\n",
    "    ax2.imshow(out_img, cmap='gray')\n",
    "    ax2.plot(left_fitx, ploty, color='yellow')\n",
    "    ax2.plot(right_fitx, ploty, color='yellow')\n",
    "    ax2.set_title('Marked Image', fontsize=10)\n",
    "    \n",
    "    ax3 = plt.subplot(1, 3, 3)\n",
    "    ax3.imshow(result, cmap='gray')\n",
    "    ax3.set_title('Final', fontsize=10)\n",
    "    ax3.plot(left_fitx, ploty, color='yellow', linewidth=2)\n",
    "    ax3.plot(right_fitx, ploty, color='yellow', linewidth=2)\n",
    "\n",
    "    plt.subplots_adjust(left=0., right=1, top=0.5, bottom=0.)\n",
    "    \"\"\"\n",
    "\n",
    "    # Create an image to draw the lines on\n",
    "    warp_zero = np.zeros_like(binary_warped).astype(np.uint8)\n",
    "    color_warp = np.dstack((warp_zero, warp_zero, warp_zero))\n",
    "#    ax3.plot(left_fitx, ploty, color='yellow', linewidth=2)\n",
    "#    ax3.plot(right_fitx, ploty, color='yellow', linewidth=2)\n",
    "\n",
    "    # Recast the x and y points into usable format for cv2.fillPoly()\n",
    "    pts_left = np.array([np.transpose(np.vstack([left_fitx, ploty]))])\n",
    "    pts_right = np.array([np.flipud(np.transpose(np.vstack([right_fitx, ploty])))])\n",
    "    pts = np.hstack((pts_left, pts_right))\n",
    "\n",
    "    # Draw the lane onto the warped blank image\n",
    "    cv2.fillPoly(color_warp, np.int_([pts]), (0,255, 0))\n",
    "#    showImg(color_warp)\n",
    "    color_warp = cv2.addWeighted(color_warp, .5, window_img, .5, 0)\n",
    "#    showImg(color_warp)\n",
    "\n",
    "    # Warp the blank back to original image space using inverse perspective matrix (Minv)\n",
    "    newwarp = cv2.warpPerspective(color_warp, perspective_Minv, (image.shape[1], image.shape[0])) \n",
    "    # Combine the result with the original image\n",
    "    marked_img = cv2.addWeighted(image, 1, newwarp, 0.3, 0)\n",
    "\n",
    "    curverad = int((l.left_curverad + l.right_curverad) / 2)\n",
    "    cv2.putText(marked_img,\"Radius of Curvature = \" + str(curverad) + \"m\", (50,50), cv2.FONT_HERSHEY_PLAIN, 2.5, (255,255,255))\n",
    "    if l.center_offset_m > 0:\n",
    "        offset_txt = \"Vehicle is \" + str(abs(l.center_offset_m)) + \"m right of centre\"\n",
    "    elif l.center_offset_m < 0:\n",
    "        offset_txt = \"Vehicle is \" + str(abs(l.center_offset_m)) + \"m left of centre\"\n",
    "    else:\n",
    "        offset_txt = \"Vehicle is at centre\"\n",
    "    \n",
    "    cv2.putText(marked_img,offset_txt, (50,100), cv2.FONT_HERSHEY_PLAIN, 2.5, (255,255,255))\n",
    "    \n",
    "    \n",
    "    return marked_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Calibrate Camera\n",
    "\n",
    "\"\"\"This section calibrates the camera by using chessboard images so that images can be undistorted\"\"\"\n",
    "\n",
    "def calibrate_camera():\n",
    "    # Chessboard corners\n",
    "    nx = 9\n",
    "    ny = 6\n",
    "\n",
    "    # prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)\n",
    "    objp = np.zeros((6*9,3), np.float32)\n",
    "    objp[:,:2] = np.mgrid[0:nx, 0:ny].T.reshape(-1,2)\n",
    "\n",
    "    # Arrays to store object points and image points from all the images.\n",
    "    objpoints = [] # 3d points in real world space\n",
    "    imgpoints = [] # 2d points in image plane.\n",
    "\n",
    "    # Make a list of calibration images\n",
    "    images = glob.glob('camera_cal/cal*.jpg')\n",
    "\n",
    "    # Step through the list and search for chessboard corners\n",
    "    for idx, fname in enumerate(images):\n",
    "        img = cv2.imread(fname)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        gray = grayscale(img)\n",
    "        # Find the chessboard corners\n",
    "        ret, corners = cv2.findChessboardCorners(gray, (nx,ny), None)\n",
    "        # If found, add object points, image points\n",
    "        if ret == True:\n",
    "            objpoints.append(objp)\n",
    "            imgpoints.append(corners)\n",
    "\n",
    "            # Draw and display the corners\n",
    "            cv2.drawChessboardCorners(img, (nx,ny), corners, ret)\n",
    "            write_name = 'camera_cal/corners_found'+str(idx)+'.jpg'\n",
    "            cv2.imwrite(write_name, img)\n",
    "\n",
    "    # Get chessboard image size\n",
    "    img = cv2.imread('camera_cal/calibration1.jpg')\n",
    "    img_size = (img.shape[1], img.shape[0])\n",
    "#    print(\"Chess board shape = \", img_size)\n",
    "\n",
    "    # Do camera calibration given object points and image points\n",
    "    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, img_size,None,None)\n",
    "    print(\"Camera calibrated.  mtx = \", mtx, \" dist = \", dist)\n",
    "\n",
    "    dst = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    cv2.imwrite('camera_cal/test_undist.jpg',dst)\n",
    "\n",
    "    # Visualize undistortion\n",
    "    \"\"\"\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\n",
    "    ax1.imshow(img)\n",
    "    ax1.set_title('Original Image', fontsize=20)\n",
    "    ax2.imshow(dst)\n",
    "    ax2.set_title('Undistorted Image', fontsize=20)\n",
    "    plt.show()\n",
    "    \"\"\"\n",
    "    return mtx, dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Compute Perspective Transform (M) and Inverse Transform (Minv)\n",
    "\n",
    "\"\"\" This function calculates the Perspective Transform (M) that can be applied to any images later \"\"\"\n",
    "\n",
    "def compute_perspective():\n",
    "    \n",
    "    global terrain_type\n",
    "    \n",
    "    if terrain_type == 'h':\n",
    "        reference_image = 'lane_calib_images/h1.png'\n",
    "    elif terrain_type == 'c':\n",
    "        reference_image = 'lane_calib_images/c1.png'\n",
    "    else:\n",
    "        reference_image = 'lane_calib_images/straight_lines1.jpg'\n",
    "\n",
    "    # Read in an image with straight lanes\n",
    "    img = cv2.imread(reference_image)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img_size = (img.shape[1], img.shape[0])\n",
    "\n",
    "    #Convert to gray scale\n",
    "    gray = grayscale(img)\n",
    "\n",
    "    # Undistorting using mtx and dist from camera caliberation\n",
    "    dst = undistort(img)\n",
    "   \n",
    "    #Set source and destination image co-ordinates\n",
    "    xoffset = 400 # offset for dst points\n",
    "    yoffset = 10 # offset for dst points\n",
    "\n",
    "    #Define 4 source points src = np.float32([[,],[,],[,],[,]]) from the straight lane image\n",
    "    \n",
    "    if terrain_type == 'h':\n",
    "        #src = np.float32([[510,498],[703,498],[961,683],[267,683]]) # Good for harder challenge\n",
    "        src = np.float32([[513,483],[664,483],[961,683],[267,683]]) # Best for harder challenge\n",
    "#        src = np.float32([[580,483],[875,483],[996,650],[355,650]]) # Curve for harder challenge\n",
    "    elif terrain_type == 'c':\n",
    "        src = np.float32([[629,498],[736,498],[1036,680],[353,680]]) # Best for challenge\n",
    "        #src = np.float32([[608,500],[741,500],[984,650],[397,650]]) # Good for challenge\n",
    "    else:\n",
    "#        src = np.float32([[588,455],[694,455],[998,650],[305,650]]) # Best co-ordinates for normal video\n",
    "        src = np.float32([[599,448],[682,448],[1010,658],[295,658]]) # second best for p1\n",
    "\n",
    "    # Define 4 destination points dst = np.float32([[,],[,],[,],[,]]) through trial and error\n",
    "    dst = np.float32([[xoffset, yoffset], [img_size[0]-xoffset, yoffset], \n",
    "                                     [img_size[0]-xoffset, img_size[1]-yoffset], \n",
    "                                     [xoffset, img_size[1]-yoffset]])\n",
    "    \n",
    "    # Calculate Perspective Transform\n",
    "    perspective_M = cv2.getPerspectiveTransform(src, dst)\n",
    "    \n",
    "    # Calculate Perspective Inverse Transform\n",
    "    perspective_Minv = cv2.getPerspectiveTransform(dst, src)\n",
    "    \n",
    "    print(\"Perspective M = \", perspective_M)\n",
    "    \n",
    "    # Warp the image using the perspective transform M\n",
    "    warped_image = cv2.warpPerspective(img, perspective_M, gray.shape[::-1], flags=cv2.INTER_LINEAR)\n",
    "\n",
    "    \"\"\"\n",
    "    # Visualize perspective transform\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "    f.tight_layout()\n",
    "    ax1.imshow(img)\n",
    "    ax1.set_title('Original Image', fontsize=30)\n",
    "    ax2.imshow(warped_image)\n",
    "    ax2.set_title('Undistorted and Warped Image', fontsize=30)\n",
    "    plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)\n",
    "    plt.show()\n",
    "\n",
    "    warped_image = cv2.cvtColor(warped_image, cv2.COLOR_RGB2BGR)\n",
    "    cv2.imwrite('sample_warped_image.jpg', warped_image)\n",
    "    \"\"\"\n",
    "    \n",
    "    return perspective_M, perspective_Minv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Determine the lane width using a clean image (straight line image).\n",
    "### This image will be used for calculating the pixel to meter conversion for x axis\n",
    "\n",
    "def compute_lane_width():\n",
    "\n",
    "    global terrain_type\n",
    "    \n",
    "    if terrain_type == 'h':\n",
    "        reference_image = 'lane_calib_images/h1.png'\n",
    "    elif terrain_type == 'c':\n",
    "        reference_image = 'lane_calib_images/c1.png'\n",
    "    else:\n",
    "        reference_image = 'lane_calib_images/straight_lines1.jpg'\n",
    "\n",
    "    # Read in a baseline image (straight lanes image)\n",
    "    img = cv2.imread(reference_image)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    binary_warped = get_binary_warp(img)\n",
    "#    showGrayImg(binary_warped)\n",
    "\n",
    "    # Take a histogram of the bottom quarter of the image\n",
    "    histogram = np.sum(binary_warped[binary_warped.shape[0]//4:,:], axis=0)\n",
    "#    print(histogram.shape)\n",
    "#    plt.plot(histogram)\n",
    "#    plt.show()\n",
    "\n",
    "    # Find the peak of the left and right halves of the histogram\n",
    "    # These will be the starting point for the left and right lines\n",
    "    baseline_midpoint = np.int(histogram.shape[0]/2)\n",
    "    baseline_leftx_base = np.argmax(histogram[:baseline_midpoint])\n",
    "    baseline_rightx_base = np.argmax(histogram[baseline_midpoint:]) + baseline_midpoint\n",
    "    baseline_lane_width = baseline_rightx_base - baseline_leftx_base\n",
    "    print(\"Baseline mid, left, center = \", baseline_midpoint, baseline_leftx_base, baseline_rightx_base)\n",
    "    print(\"Baeline Lane width in pixels = \", baseline_lane_width)\n",
    "    \n",
    "    return baseline_lane_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera calibrated.  mtx =  [[  1.15396093e+03   0.00000000e+00   6.69705359e+02]\n",
      " [  0.00000000e+00   1.14802495e+03   3.85656232e+02]\n",
      " [  0.00000000e+00   0.00000000e+00   1.00000000e+00]]  dist =  [[ -2.41017968e-01  -5.30720497e-02  -1.15810318e-03  -1.28318543e-04\n",
      "    2.67124302e-02]]\n",
      "Perspective M =  [[ -3.79366668e-01  -1.50060593e+00   8.73272564e+02]\n",
      " [ -2.77555756e-15  -1.90744658e+00   8.53880078e+02]\n",
      " [ -4.66206934e-18  -2.37856879e-03   1.00000000e+00]]\n",
      "Baseline mid, left, center =  640 398 879\n",
      "Baeline Lane width in pixels =  481\n",
      "[MoviePy] >>>> Building video project_video_output.mp4\n",
      "[MoviePy] Writing video project_video_output.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊| 1260/1261 [39:44<00:02,  2.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: project_video_output.mp4 \n",
      "\n",
      "Wall time: 39min 45s\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables\n",
    "reset_lanes = True\n",
    "detected_lines = []\n",
    "\n",
    "drive_log = open(\"p_driveLog.txt\",\"w\") \n",
    "drive_log.write(\"Driver's Log -------------------\\n\")\n",
    "\n",
    "# use n for normal, c for challenge and h for harder\n",
    "terrain_type = 'n'\n",
    "\n",
    "# Calibrate Camera\n",
    "mtx, dist = calibrate_camera()\n",
    "\n",
    "# Get perspective\n",
    "perspective_M, perspective_Minv = compute_perspective()\n",
    "\n",
    "# Get baseline lane width\n",
    "baseline_lane_width = compute_lane_width()\n",
    "\n",
    "\n",
    "image = cv2.imread('test_images\\\\test6*.jpg')\n",
    "tracker = []\n",
    "track = car_tracker()\n",
    "track.frame_number = 1\n",
    "track.cars_located = 0\n",
    "track.detected_car_positions = []\n",
    "track.heat = np.zeros_like(image).astype(np.float)\n",
    "tracker.append(track)\n",
    "\n",
    "test_video_output = 'project_video_output.mp4'\n",
    "clip2 = VideoFileClip('project_video.mp4')\n",
    "#clip2 = VideoFileClip('t4.mp4')\n",
    "video_clip = clip2.fl_image(process_image)\n",
    "%time video_clip.write_videofile(test_video_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
