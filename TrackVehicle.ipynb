{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Self-Driving Car Engineer Nanodegree\n",
    "\n",
    "\n",
    "## Project: **Vehicle Detection and Tracking** \n",
    "***\n",
    "\n",
    "This jupyter notebook contains code to train a classifier to identify car images taken from the front camera and apply to classifier to detect and track cars in a video stream\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Import necessary packages\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import time\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from skimage.feature import hog\n",
    "from support_functions import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Extract training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Extract training data\n",
    "\n",
    "# Read in cars and notcars\n",
    "cars = glob.glob('training_images\\\\vehicles\\\\**\\\\*.png', recursive=True)\n",
    "notcars = glob.glob('training_images\\\\non-vehicles\\\\**\\\\*.png', recursive=True)\n",
    "training_set = cars + notcars\n",
    "print(\"Number of car images = \", len(cars))\n",
    "print(\"Number of non car images = \", len(notcars))\n",
    "print(\"Total number of images in training set = \", len(training_set))\n",
    "print(\" \")\n",
    "\n",
    "# Define the labels vector\n",
    "y = np.hstack((np.ones(len(cars)), np.zeros(len(notcars))))\n",
    "\n",
    "# Check if the training dataset is distributed evenly across car and not-car images\n",
    "labels = 'Cars', 'Non Cars'\n",
    "sizes = [len(cars), len(notcars)]\n",
    "explode = (0, 0.05)\n",
    "p, texts, autotexts = plt.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "plt.title('Vehicle Image distribution', bbox={'facecolor':'0.8', 'pad':5})\n",
    "texts[0].set_fontsize(20)\n",
    "texts[1].set_fontsize(20)\n",
    "autotexts[0].set_fontsize(15)\n",
    "autotexts[1].set_fontsize(15)\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "if 0.5 < (len(cars)/len(notcars)) < 2:\n",
    "    print(\"**********  Training data is balanced  **********\")\n",
    "else:\n",
    "    print(\"**********  Training data is not balanced  **********\")\n",
    "print(\" \")\n",
    "    \n",
    "# Do a sanity check by picking random images to make sure data is not corrupt\n",
    "i = 500\n",
    "print(cars[i])\n",
    "image = cv2.imread(cars[i])\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "print(notcars[i])\n",
    "image = cv2.imread(notcars[i])\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Define feature extraction parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Define feature extraction parameters\n",
    "\n",
    "color_space = 'YCrCb'       # Can be RGB, HSV, LUV, HLS, YUV, YCrCb\n",
    "orient = 9                # HOG orientations\n",
    "pix_per_cell = 8          # HOG pixels per cell\n",
    "cell_per_block = 2        # HOG cells per block\n",
    "hog_channel = \"ALL\"       # Can be 0, 1, 2, or \"ALL\"\n",
    "spatial_size = (32, 32)   # Spatial binning dimensions\n",
    "hist_bins = 32            # Number of histogram bins\n",
    "spatial_feat = True       # Spatial features on or off\n",
    "hist_feat = True          # Histogram features on or off\n",
    "hog_feat = True           # HOG features on or off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Extract features for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Extract features\n",
    "spatial_features_sample = []\n",
    "color_features_sample = []\n",
    "hog_features_sample = []\n",
    "def extract_features(imgs, color_space='RGB', spatial_size=(32, 32),\n",
    "                        hist_bins=32, orient=9, \n",
    "                        pix_per_cell=8, cell_per_block=2, hog_channel=0,\n",
    "                        spatial_feat=True, hist_feat=True, hog_feat=True):\n",
    "    global spatial_features_sample, color_features_sample, hog_features_sample\n",
    "    # Start timer\n",
    "    t=time.time()\n",
    "    # Create a list to append feature vectors to\n",
    "    features = []\n",
    "    # Iterate through the list of images\n",
    "    for file in imgs:\n",
    "        file_features = []\n",
    "        # Read in each one by one\n",
    "        image = cv2.imread(file)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        # apply color conversion if other than 'RGB'\n",
    "        if color_space != 'RGB':\n",
    "            feature_image = convert_color(image, color_space)\n",
    "        else: feature_image = np.copy(image)      \n",
    "\n",
    "        if spatial_feat == True:\n",
    "            spatial_features = bin_spatial(feature_image, size=spatial_size)\n",
    "            file_features.append(spatial_features)\n",
    "            spatial_features_sample = spatial_features\n",
    "        if hist_feat == True:\n",
    "            # Apply color_hist()\n",
    "            hist_features = color_hist(feature_image, nbins=hist_bins)\n",
    "            file_features.append(hist_features)\n",
    "            color_features_sample = hist_features\n",
    "        if hog_feat == True:\n",
    "        # Call get_hog_features() with vis=False, feature_vec=True\n",
    "            if hog_channel == 'ALL':\n",
    "                hog_features = []\n",
    "                for channel in range(feature_image.shape[2]):\n",
    "                    hog_features.append(get_hog_features(feature_image[:,:,channel], \n",
    "                                        orient, pix_per_cell, cell_per_block, \n",
    "                                        vis=False, feature_vec=True))\n",
    "                hog_features = np.ravel(hog_features)        \n",
    "            else:\n",
    "                hog_features = get_hog_features(feature_image[:,:,hog_channel], orient, \n",
    "                            pix_per_cell, cell_per_block, vis=False, feature_vec=True)\n",
    "            # Append the new feature vector to the features list\n",
    "            file_features.append(hog_features)\n",
    "            hog_features_sample = hog_features\n",
    "        features.append(np.concatenate(file_features))\n",
    "    # Stop timer\n",
    "    t2 = time.time()\n",
    "    print(round(t2-t, 2), 'Seconds to extract features...')\n",
    "    # Return list of feature vectors\n",
    "    return features\n",
    "\n",
    "features = extract_features(training_set, color_space=color_space, \n",
    "                        spatial_size=spatial_size, hist_bins=hist_bins, \n",
    "                        orient=orient, pix_per_cell=pix_per_cell, \n",
    "                        cell_per_block=cell_per_block, \n",
    "                        hog_channel=hog_channel, spatial_feat=spatial_feat, \n",
    "                        hist_feat=hist_feat, hog_feat=hog_feat)\n",
    "\n",
    "# Fit a per-column scaler\n",
    "X_scaler = StandardScaler().fit(features)\n",
    "# Apply the scaler to X\n",
    "scaled_X = X_scaler.transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(len(spatial_features_sample))\n",
    "print(len(color_features_sample))\n",
    "print(len(hog_features_sample))\n",
    "print(len(features[0]))\n",
    "print(len(features))\n",
    "print(X_scaler)\n",
    "print(len(scaled_X))\n",
    "print(scaled_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "###  Train the model\n",
    "\n",
    "# Split up data into randomized training and test sets\n",
    "rand_state = np.random.randint(0, 100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    scaled_X, y, test_size=0.2, random_state=rand_state)\n",
    "\n",
    "print('Using:',orient,'orientations',pix_per_cell, 'pixels per cell and', cell_per_block,'cells per block')\n",
    "print('Feature vector length:', len(X_train[0]))\n",
    "\n",
    "# Use a linear SVC \n",
    "svc = LinearSVC()\n",
    "# Check the training time for the SVC\n",
    "t=time.time()\n",
    "svc.fit(X_train, y_train)\n",
    "t2 = time.time()\n",
    "print(round(t2-t, 2), 'Seconds to train SVC...')\n",
    "# Check the score of the SVC\n",
    "print('Test Accuracy of SVC = ', round(svc.score(X_test, y_test), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Save model and feature extraction parameters to pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Save fit and feature extraction parameters\n",
    "\n",
    "pickle_file = 'VDT_pickle.p'\n",
    "print('Saving data to pickle file...')\n",
    "try:\n",
    "    with open('VDT_pickle.p', 'wb') as pfile:\n",
    "        pickle.dump(\n",
    "            {\n",
    "                'svc': svc,\n",
    "                'X_scaler': X_scaler,\n",
    "                'color_space': color_space,\n",
    "                'hog_channel': hog_channel,\n",
    "                'orient': orient,\n",
    "                'pix_per_cell': pix_per_cell,\n",
    "                'cell_per_block': cell_per_block,\n",
    "                'spatial_size': spatial_size,\n",
    "                'hist_bins': hist_bins,\n",
    "            },\n",
    "            pfile, pickle.HIGHEST_PROTOCOL)\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', pickle_file, ':', e)\n",
    "    raise\n",
    "print(\"Pickle file saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Identifying vehicles using the trained classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import cv2\n",
    "from support_functions import *\n",
    "from scipy.ndimage.measurements import label\n",
    "import glob\n",
    "\n",
    "class car_tracker():\n",
    "    def __init__(self):\n",
    "        self.frame_number = None\n",
    "        self.cars_located = None\n",
    "        self.detected_car_positions = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Load the classifier and feature extraction parameters from the pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svc =  LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "scaler =  StandardScaler(copy=True, with_mean=True, with_std=True)\n",
      "color space =  YCrCb\n",
      "hog channel =  ALL\n",
      "orient =  9\n",
      "pix_per_cell =  8\n",
      "cell_per_block =  2\n",
      "spatial_size =  (32, 32)\n",
      "hist_bins =  32\n"
     ]
    }
   ],
   "source": [
    "dist_pickle = pickle.load( open(\"VDT_pickle.p\", \"rb\" ) )\n",
    "svc = dist_pickle[\"svc\"]\n",
    "X_scaler = dist_pickle[\"X_scaler\"]\n",
    "color_space = dist_pickle[\"color_space\"]\n",
    "hog_channel = dist_pickle[\"hog_channel\"]\n",
    "orient = dist_pickle[\"orient\"]\n",
    "pix_per_cell = dist_pickle[\"pix_per_cell\"]\n",
    "cell_per_block = dist_pickle[\"cell_per_block\"]\n",
    "spatial_size = dist_pickle[\"spatial_size\"]\n",
    "hist_bins = dist_pickle[\"hist_bins\"]\n",
    "\n",
    "print(\"svc = \", svc)\n",
    "print(\"scaler = \", X_scaler)\n",
    "print(\"color space = \", color_space)\n",
    "print(\"hog channel = \", hog_channel)\n",
    "print(\"orient = \", orient)\n",
    "print(\"pix_per_cell = \", pix_per_cell)\n",
    "print(\"cell_per_block = \", cell_per_block)\n",
    "print(\"spatial_size = \", spatial_size)\n",
    "print(\"hist_bins = \", hist_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define a single function that can extract features using hog sub-sampling and make predictions\n",
    "def find_cars(img, ystart, ystop, scale, svc, X_scaler, orient, pix_per_cell, cell_per_block, spatial_size, hist_bins, color_space):\n",
    "    \n",
    "    bboxes = []\n",
    "    draw_img = np.copy(img)\n",
    "#    img = img.astype(np.float32)/255\n",
    "\n",
    "    img_tosearch = img[ystart:ystop,:,:]\n",
    "    ctrans_tosearch = convert_color(img_tosearch, color_space)\n",
    "#    ctrans_tosearch = cv2.cvtColor(img_tosearch, cv2.COLOR_RGB2YCrCb)\n",
    "    if scale != 1:\n",
    "        imshape = ctrans_tosearch.shape\n",
    "        ctrans_tosearch = cv2.resize(ctrans_tosearch, (np.int(imshape[1]/scale), np.int(imshape[0]/scale)))\n",
    "        \n",
    "    ch1 = ctrans_tosearch[:,:,0]\n",
    "    ch2 = ctrans_tosearch[:,:,1]\n",
    "    ch3 = ctrans_tosearch[:,:,2]\n",
    "\n",
    "    # Define blocks and steps as above\n",
    "#    print(\"ch1.shape[1] = \", ch1.shape[1])\n",
    "    nxblocks = (ch1.shape[1] // pix_per_cell) - cell_per_block + 1\n",
    "#    print(\"nxblocks =\", nxblocks)\n",
    "    nyblocks = (ch1.shape[0] // pix_per_cell) - cell_per_block + 1 \n",
    "#    print(\"nyblocks =\", nyblocks)\n",
    "    nfeat_per_block = orient*cell_per_block**2\n",
    "#    print(\"nfeatpb =\", nfeat_per_block)\n",
    "    \n",
    "    # 64 was the orginal sampling rate, with 8 cells and 8 pix per cell\n",
    "    window = 64\n",
    "    nblocks_per_window = (window // pix_per_cell) - cell_per_block + 1\n",
    "#    print(\"nblocks_per_window = \", nblocks_per_window)\n",
    "    cells_per_step = 2  # Instead of overlap, define how many cells to step\n",
    "    nxsteps = (nxblocks - nblocks_per_window) // cells_per_step\n",
    "    nysteps = (nyblocks - nblocks_per_window) // cells_per_step\n",
    "#    print(\"nxsteps = \", nxsteps)\n",
    "#    print(\"nysteps = \", nysteps)\n",
    "    \n",
    "    # Compute individual channel HOG features for the entire image\n",
    "    hog1 = get_hog_features(ch1, orient, pix_per_cell, cell_per_block, feature_vec=False)\n",
    "    hog2 = get_hog_features(ch2, orient, pix_per_cell, cell_per_block, feature_vec=False)\n",
    "    hog3 = get_hog_features(ch3, orient, pix_per_cell, cell_per_block, feature_vec=False)\n",
    "    \n",
    "    for xb in range(nxsteps):\n",
    "        for yb in range(nysteps):\n",
    "            ypos = yb*cells_per_step\n",
    "            xpos = xb*cells_per_step\n",
    "            # Extract HOG for this patch\n",
    "            hog_feat1 = hog1[ypos:ypos+nblocks_per_window, xpos:xpos+nblocks_per_window].ravel() \n",
    "            hog_feat2 = hog2[ypos:ypos+nblocks_per_window, xpos:xpos+nblocks_per_window].ravel() \n",
    "            hog_feat3 = hog3[ypos:ypos+nblocks_per_window, xpos:xpos+nblocks_per_window].ravel() \n",
    "            hog_features = np.hstack((hog_feat1, hog_feat2, hog_feat3))\n",
    "\n",
    "            xleft = xpos*pix_per_cell\n",
    "            ytop = ypos*pix_per_cell\n",
    "\n",
    "            # Extract the image patch\n",
    "            subimg = cv2.resize(ctrans_tosearch[ytop:ytop+window, xleft:xleft+window], (64,64))\n",
    "          \n",
    "            # Get color features\n",
    "            spatial_features = bin_spatial(subimg, size=spatial_size)\n",
    "            hist_features = color_hist(subimg, nbins=hist_bins)\n",
    "\n",
    "            # Fit a per-column scaler\n",
    "            test_features = X_scaler.transform(np.hstack((spatial_features, hist_features, hog_features)).reshape(1, -1))    \n",
    "            # Scale features and make a prediction\n",
    "            test_prediction = svc.predict(test_features)\n",
    "            \n",
    "            if test_prediction == 1:\n",
    "                xbox_left = np.int(xleft*scale)\n",
    "                ytop_draw = np.int(ytop*scale)\n",
    "                win_draw = np.int(window*scale)\n",
    "#                cv2.rectangle(draw_img,(xbox_left, ytop_draw+ystart),(xbox_left+win_draw,ytop_draw+win_draw+ystart),(0,0,255),6) \n",
    "                box = [(xbox_left, ytop_draw+ystart), (xbox_left+win_draw,ytop_draw+win_draw+ystart)]\n",
    "                bboxes.append(box)\n",
    "                \n",
    "    return bboxes \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def detection_pipeline(img):\n",
    "    ystart = 400\n",
    "    ystop = 656\n",
    "    scale_list = [1, 1.5, 2, 2.5]\n",
    "    bboxes = []\n",
    "    heat_threshold = 4\n",
    "    global track\n",
    "\n",
    "    for scale in scale_list:\n",
    "        detected_boxes = find_cars(img, ystart, ystop, scale, svc, X_scaler, orient, pix_per_cell, cell_per_block, \n",
    "                            spatial_size, hist_bins, color_space)\n",
    "        bboxes.extend(detected_boxes)\n",
    "\n",
    "    out_img = np.copy(img)\n",
    "    for box in bboxes:\n",
    "        cv2.rectangle(out_img, box[0], box[1],(0,0,255),6) \n",
    "#    showImg(out_img)\n",
    "    cv2.imwrite(\"raw_img.jpg\",out_img)\n",
    "\n",
    "    heat = np.zeros_like(img[:,:,0]).astype(np.float)\n",
    "\n",
    "    # Add heat to each box in box list\n",
    "    heat = add_heat(heat,bboxes)\n",
    "    \"\"\"    \n",
    "    # Add heat weight for zone around previous detections\n",
    "    for pos in tracker[-1].detected_car_positions:\n",
    "        min_x = pos[0][0]\n",
    "        min_y = pos[0][1]\n",
    "        max_x = pos[1][0]\n",
    "        max_y = pos[1][1]\n",
    "        \n",
    "        min_x = np.clip(min_x - 5, 0, heat.shape[1])\n",
    "        max_x = np.clip(min_x + 5, 0, heat.shape[1])\n",
    "        min_y = np.clip(min_y - 5, 0, heat.shape[0])\n",
    "        max_y = np.clip(min_y + 5, 0, heat.shape[0])\n",
    "        \n",
    "        for x in range(min_x, max_x):\n",
    "            for y in range (min_y, max_y):\n",
    "                if 0 < heat[y,x] < heat_threshold:\n",
    "                    heat[y,x] += 1\n",
    "    \"\"\"                    \n",
    "#    print(\"box details = \", tracker[-1].detected_car_positions[0])\n",
    "    # Apply threshold to help remove false positives\n",
    "    heat = apply_threshold(heat, heat_threshold)\n",
    "\n",
    "    # Visualize the heatmap when displaying    \n",
    "    heatmap = np.clip(heat, 0, 255)\n",
    "    \n",
    "\n",
    "    # Find final boxes from heatmap using label function\n",
    "    labels = label(heatmap)\n",
    "    draw_img, final_bboxes = draw_labeled_bboxes(np.copy(img), labels)\n",
    "    track.cars_located = labels\n",
    "    track.detected_car_positions = final_bboxes\n",
    "    cv2.putText(draw_img,\"Frame Number  = \" + str(track.frame_number), (900,50), cv2.FONT_HERSHEY_PLAIN, 2, (255,255,255))\n",
    "    cv2.putText(draw_img,\"Cars detected =  \" + str(labels[1]), (900,80), cv2.FONT_HERSHEY_PLAIN, 2, (255,255,255))\n",
    "    \n",
    "    return draw_img, heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tracker = []\n",
    "track = car_tracker()\n",
    "track.frame_number = 1\n",
    "track.cars_located = 0\n",
    "track.detected_car_positions = []\n",
    "tracker.append(track)\n",
    "\n",
    "test_images = glob.glob('test_images\\\\test6*.jpg')\n",
    "for img in test_images:\n",
    "    img = cv2.imread(img)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    draw_img, heatmap = detection_pipeline(img)\n",
    "    fig = plt.figure()\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(draw_img)\n",
    "    plt.title('Car Positions')\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(heatmap, cmap='hot')\n",
    "    plt.title('Heat Map')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    draw_img = cv2.cvtColor(draw_img, cv2.COLOR_RGB2BGR)\n",
    "    cv2.imwrite(\"marked_img.jpg\",draw_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import everything needed to edit/save/watch video clips\n",
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "def process_image(image):  \n",
    "    global track\n",
    "    track.frame_number = tracker[-1].frame_number + 1\n",
    "    result, heat_map = detection_pipeline(image)\n",
    "    tracker.append(track)\n",
    "    track = car_tracker()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video test_video_output.mp4\n",
      "[MoviePy] Writing video test_video_output.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉     | 38/39 [00:58<00:01,  1.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: test_video_output.mp4 \n",
      "\n",
      "Wall time: 59.7 s\n"
     ]
    }
   ],
   "source": [
    "tracker = []\n",
    "track = car_tracker()\n",
    "track.frame_number = 0\n",
    "track.cars_located = 0\n",
    "track.detected_car_positions = []\n",
    "tracker.append(track)\n",
    "track = car_tracker()\n",
    "test_video_output = 'test_video_output.mp4'\n",
    "clip2 = VideoFileClip('test_video.mp4')\n",
    "video_clip = clip2.fl_image(process_image)\n",
    "%time video_clip.write_videofile(test_video_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "project_video_output = 'project_video_output.mp4'\n",
    "clip2 = VideoFileClip('project_video.mp4')\n",
    "video_clip = clip2.fl_image(process_image)\n",
    "%time video_clip.write_videofile(project_video_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
